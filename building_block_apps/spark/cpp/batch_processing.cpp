#include "spark_common.h"

/**
 * Spark Batch Processing Examples - Building Block Application (C++)
 * 
 * This application demonstrates C++ integration with Spark batch processing:
 * - High-performance data generation in C++
 * - Integration with Spark via data files and spark-submit
 * - Performance monitoring and data quality checks
 * - Multiple output formats for Spark consumption
 * 
 * Usage:
 *   bazel run //spark/cpp:batch_processing -- --records 50000 --output-path /tmp/batch-results
 * 
 * Examples:
 *   # Basic batch processing
 *   bazel run //spark/cpp:batch_processing
 *   
 *   # Large dataset generation
 *   bazel run //spark/cpp:batch_processing -- --records 1000000
 *   
 *   # Custom output location
 *   bazel run //spark/cpp:batch_processing -- --output-path /tmp/my-results
 */

using namespace SparkCommon;

/**
 * Generate and save sample data for Spark processing.
 */
void generateAndSaveData(const CommandLineArgs& args) {
    LOG_INFO("ðŸ”„ Step 1: Generating sample data...");
    
    PerformanceMetrics dataGenMetrics("Data Generation");
    
    // Generate sample data using high-performance C++
    auto data = generateSampleData(args.records);
    dataGenMetrics.finish(args.records);
    
    // Create output directory
    createDirectory(args.outputPath);
    createDirectory(args.outputPath + "/input");
    
    // Save data in multiple formats for Spark
    std::string csvFile = args.outputPath + "/input/sample_data.csv";
    std::string jsonFile = args.outputPath + "/input/sample_data.json";
    
    LOG_INFO("   Saving data files...");
    saveSampleDataToCSV(data, csvFile);
    saveSampleDataToJSON(data, jsonFile);
    
    // Print data quality summary
    printDataQualitySummary(data);
    
    LOG_INFO("âœ… Data generation completed");
    LOG_INFO("   CSV file: " + csvFile);
    LOG_INFO("   JSON file: " + jsonFile);
}

/**
 * Create a Python script for Spark processing.
 */
void createSparkScript(const CommandLineArgs& args) {
    LOG_INFO("ðŸ”„ Step 2: Creating Spark processing script...");
    
    std::string scriptPath = args.outputPath + "/spark_batch_processor.py";
    std::ofstream script(scriptPath);
    
    if (!script.is_open()) {
        throw SparkException("Cannot create Spark script: " + scriptPath);
    }
    
    // Write Python script for Spark processing
    script << R"(#!/usr/bin/env python3
"""
Spark Batch Processing Script (Generated by C++ Application)
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import sys

def main():
    # Create Spark session
    spark = SparkSession.builder \
        .appName(")" << args.appName << R"(") \
        .getOrCreate()
    
    input_path = ")" << args.outputPath << R"(/input/sample_data.csv"
    output_path = ")" << args.outputPath << R"(/output"
    
    print("ðŸ”„ Loading data from: " + input_path)
    
    # Load CSV data
    df = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .csv(input_path)
    
    print(f"   Loaded {df.count()} records")
    
    # Data transformations
    print("ðŸ”„ Performing transformations...")
    
    transformed_df = df \
        .withColumn("total_amount", col("price") * col("quantity")) \
        .withColumn("date", date_format(col("timestamp"), "yyyy-MM-dd")) \
        .withColumn("hour", hour(col("timestamp"))) \
        .filter(col("price") > 0) \
        .filter(col("quantity") > 0)
    
    print(f"   Transformed to {transformed_df.count()} records")
    
    # Aggregations
    print("ðŸ”„ Computing aggregations...")
    
    # Sales by category
    category_sales = transformed_df.groupBy("category") \
        .agg(
            count("*").alias("order_count"),
            sum("total_amount").alias("total_revenue"),
            avg("total_amount").alias("avg_order_value")
        ) \
        .orderBy(desc("total_revenue"))
    
    print("   Category Sales:")
    category_sales.show()
    
    # Daily trends
    daily_trends = transformed_df.groupBy("date") \
        .agg(
            count("*").alias("daily_orders"),
            sum("total_amount").alias("daily_revenue"),
            countDistinct("user_id").alias("unique_customers")
        ) \
        .orderBy("date")
    
    print("   Daily Trends:")
    daily_trends.show(10)
    
    # Save results
    print("ðŸ”„ Saving results...")
    
    # Save transformed data
    transformed_df.write \
        .mode("overwrite") \
        .partitionBy("category", "date") \
        .parquet(output_path + "/transformed_data")
    
    # Save aggregations
    category_sales.coalesce(1) \
        .write \
        .mode("overwrite") \
        .option("header", "true") \
        .csv(output_path + "/category_sales")
    
    daily_trends.coalesce(1) \
        .write \
        .mode("overwrite") \
        .option("header", "true") \
        .csv(output_path + "/daily_trends")
    
    print("âœ… Spark processing completed")
    print(f"   Results saved to: {output_path}")
    
    spark.stop()

if __name__ == "__main__":
    main()
)";
    
    script.close();
    
    // Make script executable
    std::string chmodCmd = "chmod +x " + scriptPath;
    std::system(chmodCmd.c_str());
    
    LOG_INFO("âœ… Created Spark script: " + scriptPath);
}

/**
 * Execute Spark processing job.
 */
void executeSparkProcessing(const CommandLineArgs& args) {
    LOG_INFO("ðŸ”„ Step 3: Executing Spark processing...");
    
    SparkConfig config = createSparkConfig(args);
    std::string scriptPath = args.outputPath + "/spark_batch_processor.py";
    
    // Check if Spark cluster is accessible
    if (!checkSparkCluster(config.master)) {
        LOG_WARN("Spark cluster not accessible, using local mode");
        config.master = "local[*]";
    }
    
    PerformanceMetrics sparkMetrics("Spark Processing");
    
    // Execute Spark job
    int result = executeSparkJob(config, scriptPath);
    
    sparkMetrics.finish();
    
    if (result != 0) {
        throw SparkException("Spark job failed with exit code: " + std::to_string(result));
    }
    
    LOG_INFO("âœ… Spark processing completed successfully");
}

/**
 * Analyze results and print summary.
 */
void analyzeResults(const CommandLineArgs& args) {
    LOG_INFO("ðŸ”„ Step 4: Analyzing results...");
    
    std::string outputPath = args.outputPath + "/output";
    
    // Check if output files exist
    std::vector<std::string> expectedFiles = {
        outputPath + "/transformed_data",
        outputPath + "/category_sales",
        outputPath + "/daily_trends"
    };
    
    std::cout << "\nðŸ“Š BATCH PROCESSING RESULTS SUMMARY\n";
    std::cout << "====================================\n";
    
    std::cout << "\nðŸ“ˆ PROCESSING METRICS:\n";
    std::cout << "   Input Records: " << args.records << "\n";
    std::cout << "   Output Path: " << outputPath << "\n";
    
    std::cout << "\nðŸ“ OUTPUT FILES:\n";
    for (const auto& file : expectedFiles) {
        struct stat buffer;
        bool exists = (stat(file.c_str(), &buffer) == 0);
        std::cout << "   " << (exists ? "âœ…" : "âŒ") << " " << file << "\n";
    }
    
    std::cout << "\nðŸ”— INTEGRATION POINTS:\n";
    std::cout << "   Data Generation: High-performance C++ (native)\n";
    std::cout << "   Data Processing: Spark cluster (distributed)\n";
    std::cout << "   Output Formats: Parquet (columnar), CSV (interop)\n";
    
    std::cout << "\nðŸ’¡ NEXT STEPS:\n";
    std::cout << "   - View results in Spark UI: http://192.168.1.184:4040\n";
    std::cout << "   - Analyze Parquet files with Spark, Pandas, or other tools\n";
    std::cout << "   - Use CSV files for visualization or reporting tools\n";
    
    std::cout << "\n====================================\n";
    
    LOG_INFO("âœ… Analysis completed");
}

/**
 * Main batch processing workflow.
 */
int main(int argc, char* argv[]) {
    try {
        // Parse command line arguments
        CommandLineArgs args = parseCommandLineArgs(argc, argv);
        
        if (args.help) {
            printUsage(argv[0]);
            return 0;
        }
        
        LOG_INFO("ðŸš€ Starting C++ Spark Batch Processing Example");
        LOG_INFO("   Records to generate: " + std::to_string(args.records));
        LOG_INFO("   Output path: " + args.outputPath);
        LOG_INFO("   Spark master: " + args.master);
        
        PerformanceMetrics totalMetrics("Total Processing");
        
        // Execute processing pipeline
        generateAndSaveData(args);
        createSparkScript(args);
        executeSparkProcessing(args);
        analyzeResults(args);
        
        totalMetrics.finish(args.records);
        
        LOG_INFO("âœ… C++ Spark batch processing completed successfully!");
        LOG_INFO("   Generated " + std::to_string(args.records) + " records");
        LOG_INFO("   Results available in: " + args.outputPath);
        
        return 0;
        
    } catch (const SparkException& e) {
        LOG_ERROR("Spark error: " + std::string(e.what()));
        return 1;
    } catch (const std::exception& e) {
        LOG_ERROR("Error: " + std::string(e.what()));
        return 1;
    } catch (...) {
        LOG_ERROR("Unknown error occurred");
        return 1;
    }
}
