#include "spark_common.h"

/**
 * Spark ETL Pipeline Examples - Building Block Application (C++)
 * 
 * This application demonstrates C++ integration with Spark ETL pipelines:
 * - High-performance data extraction and preparation
 * - Complex ETL workflow orchestration
 * - Data quality validation and monitoring
 * - Multiple output formats and destinations
 * 
 * Usage:
 *   bazel run //spark/cpp:etl_pipeline -- --records 50000 --output-path /tmp/etl-results
 * 
 * Examples:
 *   # Basic ETL pipeline
 *   bazel run //spark/cpp:etl_pipeline
 *   
 *   # Large-scale ETL
 *   bazel run //spark/cpp:etl_pipeline -- --records 1000000 --output-path /tmp/warehouse
 */

using namespace SparkCommon;

/**
 * Generate multi-source data for ETL processing.
 */
void generateETLSources(const CommandLineArgs& args) {
    LOG_INFO("🔄 EXTRACT: Generating multi-source data...");
    
    createDirectory(args.outputPath);
    createDirectory(args.outputPath + "/sources");
    
    PerformanceMetrics extractMetrics("Data Extraction");
    
    // Generate main sales data
    LOG_INFO("   Generating sales transactions...");
    auto salesData = generateSampleData(args.records);
    std::string salesFile = args.outputPath + "/sources/sales_data.csv";
    saveSampleDataToCSV(salesData, salesFile);
    
    // Generate customer data (derived from sales)
    LOG_INFO("   Generating customer data...");
    std::vector<SampleRecord> customerData;
    std::set<std::string> uniqueUsers;
    
    for (const auto& record : salesData) {
        if (uniqueUsers.find(record.userId) == uniqueUsers.end()) {
            uniqueUsers.insert(record.userId);
            // Create a customer record with synthetic data
            SampleRecord customer = record;
            customer.productId = "N/A";
            customer.category = "customer_profile";
            customer.action = "registration";
            customer.price = 0.0;
            customer.quantity = 1;
            customerData.push_back(customer);
        }
    }
    
    std::string customerFile = args.outputPath + "/sources/customer_data.csv";
    saveSampleDataToCSV(customerData, customerFile);
    
    // Generate product catalog
    LOG_INFO("   Generating product catalog...");
    std::vector<SampleRecord> productData;
    std::set<std::string> uniqueProducts;
    
    for (const auto& record : salesData) {
        if (uniqueProducts.find(record.productId) == uniqueProducts.end()) {
            uniqueProducts.insert(record.productId);
            SampleRecord product = record;
            product.userId = "N/A";
            product.action = "catalog_entry";
            product.sessionId = "N/A";
            product.quantity = 1;
            productData.push_back(product);
        }
    }
    
    std::string productFile = args.outputPath + "/sources/product_data.csv";
    saveSampleDataToCSV(productData, productFile);
    
    extractMetrics.finish(salesData.size() + customerData.size() + productData.size());
    
    LOG_INFO("✅ Data extraction completed:");
    LOG_INFO("   Sales records: " + std::to_string(salesData.size()));
    LOG_INFO("   Customer records: " + std::to_string(customerData.size()));
    LOG_INFO("   Product records: " + std::to_string(productData.size()));
}

/**
 * Create comprehensive ETL script.
 */
void createETLScript(const CommandLineArgs& args) {
    LOG_INFO("🔄 TRANSFORM: Creating ETL processing script...");
    
    std::string scriptPath = args.outputPath + "/spark_etl_pipeline.py";
    std::ofstream script(scriptPath);
    
    if (!script.is_open()) {
        throw SparkException("Cannot create ETL script: " + scriptPath);
    }
    
    script << R"(#!/usr/bin/env python3
"""
Spark ETL Pipeline Script (Generated by C++ Application)
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import sys
import json
import time

def main():
    # Create Spark session with ETL optimizations
    spark = SparkSession.builder \
        .appName(")" << args.appName << R"(_ETL_Pipeline") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    
    sources_path = ")" << args.outputPath << R"(/sources"
    output_path = ")" << args.outputPath << R"(/etl_output"
    
    start_time = time.time()
    
    print("🔄 ETL PIPELINE: Extract, Transform, Load")
    print("=" * 50)
    
    # EXTRACT: Load data from multiple sources
    print("\n🔄 EXTRACT: Loading data sources...")
    
    sales_df = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .csv(sources_path + "/sales_data.csv")
    
    customers_df = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .csv(sources_path + "/customer_data.csv") \
        .filter(col("category") == "customer_profile")
    
    products_df = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .csv(sources_path + "/product_data.csv") \
        .filter(col("action") == "catalog_entry")
    
    print(f"   Sales: {sales_df.count()} records")
    print(f"   Customers: {customers_df.count()} records")
    print(f"   Products: {products_df.count()} records")
    
    # TRANSFORM: Apply business logic and data transformations
    print("\n🔄 TRANSFORM: Applying transformations...")
    
    # 1. Sales fact table transformations
    print("   Transforming sales data...")
    sales_fact = sales_df \
        .withColumn("sale_date", date_format(col("timestamp"), "yyyy-MM-dd")) \
        .withColumn("sale_hour", hour(col("timestamp"))) \
        .withColumn("total_amount", col("price") * col("quantity")) \
        .withColumn("discount_amount", 
                   when(col("total_amount") > 1000, col("total_amount") * 0.1)
                   .when(col("total_amount") > 500, col("total_amount") * 0.05)
                   .otherwise(0)) \
        .withColumn("final_amount", col("total_amount") - col("discount_amount")) \
        .withColumn("profit_margin", 
                   when(col("category") == "electronics", 0.15)
                   .when(col("category") == "clothing", 0.40)
                   .when(col("category") == "books", 0.20)
                   .when(col("category") == "home", 0.25)
                   .otherwise(0.30)) \
        .withColumn("estimated_profit", col("final_amount") * col("profit_margin")) \
        .withColumn("order_id", concat(lit("ORD-"), 
                                     date_format(col("timestamp"), "yyyyMMdd"), 
                                     lit("-"), 
                                     abs(hash(col("user_id"), col("timestamp"))))) \
        .filter(col("price") > 0) \
        .filter(col("quantity") > 0)
    
    # 2. Customer dimension transformations
    print("   Transforming customer data...")
    customer_dim = customers_df \
        .withColumn("customer_id", regexp_replace(col("user_id"), "user_", "cust_")) \
        .withColumn("full_name", concat(lit("Customer "), 
                                       regexp_extract(col("user_id"), "(\\d+)", 1))) \
        .withColumn("email", concat(lower(col("user_id")), lit("@example.com"))) \
        .withColumn("registration_date", 
                   date_sub(current_date(), 
                           regexp_extract(col("user_id"), "(\\d+)", 1).cast("int") % 365)) \
        .withColumn("customer_segment", 
                   when(col("is_premium"), "Premium")
                   .when(col("region").isin(["US-East", "US-West"]), "Domestic")
                   .otherwise("International")) \
        .select("customer_id", "user_id", "full_name", "email", "region", 
                "registration_date", "customer_segment", "is_premium")
    
    # 3. Product dimension transformations
    print("   Transforming product data...")
    product_dim = products_df \
        .withColumn("sku", concat(lit("SKU-"), 
                                 regexp_extract(col("product_id"), "(\\d+)", 1))) \
        .withColumn("product_name", concat(lit("Product "), 
                                          regexp_extract(col("product_id"), "(\\d+)", 1))) \
        .withColumn("cost_price", col("price") * 0.7) \
        .withColumn("markup_percentage", round((col("price") - col("cost_price")) / col("cost_price") * 100, 2)) \
        .withColumn("price_tier",
                   when(col("price") >= 500, "Premium")
                   .when(col("price") >= 100, "Standard")
                   .otherwise("Budget")) \
        .select("product_id", "sku", "product_name", "category", "price", 
                "cost_price", "markup_percentage", "price_tier")
    
    # 4. Create aggregated tables
    print("   Creating aggregated dimensions...")
    
    # Daily sales summary
    daily_sales = sales_fact \
        .groupBy("sale_date", "category") \
        .agg(
            count("*").alias("transaction_count"),
            sum("final_amount").alias("total_revenue"),
            sum("estimated_profit").alias("total_profit"),
            avg("final_amount").alias("avg_transaction_value"),
            countDistinct("user_id").alias("unique_customers")
        )
    
    # Customer summary
    customer_summary = sales_fact \
        .groupBy("user_id") \
        .agg(
            count("*").alias("total_orders"),
            sum("final_amount").alias("total_spent"),
            avg("final_amount").alias("avg_order_value"),
            max("timestamp").alias("last_order_date"),
            min("timestamp").alias("first_order_date"),
            collect_set("category").alias("categories_purchased")
        ) \
        .withColumn("customer_tenure_days",
                   datediff(col("last_order_date"), col("first_order_date")))
    
    print(f"   Sales fact: {sales_fact.count()} records")
    print(f"   Customer dim: {customer_dim.count()} records")
    print(f"   Product dim: {product_dim.count()} records")
    print(f"   Daily sales agg: {daily_sales.count()} records")
    print(f"   Customer summary: {customer_summary.count()} records")
    
    # DATA QUALITY VALIDATION
    print("\n🔄 VALIDATE: Performing data quality checks...")
    
    quality_issues = 0
    
    # Check for nulls in critical fields
    null_checks = {
        "sales_fact": sales_fact.filter(col("user_id").isNull() | col("product_id").isNull()).count(),
        "customer_dim": customer_dim.filter(col("customer_id").isNull()).count(),
        "product_dim": product_dim.filter(col("product_id").isNull()).count()
    }
    
    for table, null_count in null_checks.items():
        if null_count > 0:
            print(f"   ⚠️  {table}: {null_count} null values in key fields")
            quality_issues += null_count
    
    # Business rule validations
    negative_amounts = sales_fact.filter(col("final_amount") < 0).count()
    zero_quantities = sales_fact.filter(col("quantity") <= 0).count()
    invalid_prices = product_dim.filter(col("price") <= 0).count()
    
    if negative_amounts > 0:
        print(f"   ⚠️  Sales: {negative_amounts} negative amounts")
        quality_issues += negative_amounts
    if zero_quantities > 0:
        print(f"   ⚠️  Sales: {zero_quantities} zero quantities")
        quality_issues += zero_quantities
    if invalid_prices > 0:
        print(f"   ⚠️  Products: {invalid_prices} invalid prices")
        quality_issues += invalid_prices
    
    # Calculate quality score
    total_records = sales_fact.count() + customer_dim.count() + product_dim.count()
    quality_score = max(0, 100 - (quality_issues / total_records * 100)) if total_records > 0 else 100
    
    print(f"   Overall Quality Score: {quality_score:.1f}%")
    print(f"   Total Issues: {quality_issues}")
    
    # LOAD: Save transformed data
    print("\n🔄 LOAD: Saving transformed data...")
    
    # Save fact tables (partitioned)
    print("   Saving fact tables...")
    sales_fact.write \
        .mode("overwrite") \
        .partitionBy("sale_date", "category") \
        .parquet(output_path + "/fact/sales")
    
    # Save dimension tables
    print("   Saving dimension tables...")
    customer_dim.write \
        .mode("overwrite") \
        .parquet(output_path + "/dim/customers")
    
    product_dim.write \
        .mode("overwrite") \
        .parquet(output_path + "/dim/products")
    
    # Save aggregated tables
    print("   Saving aggregated tables...")
    daily_sales.write \
        .mode("overwrite") \
        .parquet(output_path + "/agg/daily_sales")
    
    customer_summary.write \
        .mode("overwrite") \
        .parquet(output_path + "/agg/customer_summary")
    
    # Save business reports (CSV)
    print("   Saving business reports...")
    
    # Category performance report
    category_report = daily_sales.groupBy("category") \
        .agg(
            sum("total_revenue").alias("category_revenue"),
            sum("transaction_count").alias("total_transactions"),
            avg("avg_transaction_value").alias("avg_transaction_value")
        )
    
    category_report.coalesce(1) \
        .write \
        .mode("overwrite") \
        .option("header", "true") \
        .csv(output_path + "/reports/category_performance")
    
    # Top customers report
    top_customers = customer_summary \
        .orderBy(col("total_spent").desc()) \
        .limit(100)
    
    top_customers.coalesce(1) \
        .write \
        .mode("overwrite") \
        .option("header", "true") \
        .csv(output_path + "/reports/top_customers")
    
    # Generate ETL report
    end_time = time.time()
    duration = end_time - start_time
    
    etl_report = {
        "pipeline_execution": {
            "start_time": start_time,
            "end_time": end_time,
            "duration_seconds": duration,
            "status": "COMPLETED"
        },
        "data_volumes": {
            "input_records": )" << args.records << R"(,
            "sales_fact_records": sales_fact.count(),
            "customer_dim_records": customer_dim.count(),
            "product_dim_records": product_dim.count(),
            "total_output_records": sales_fact.count() + customer_dim.count() + product_dim.count()
        },
        "data_quality": {
            "quality_score": quality_score,
            "total_issues": quality_issues,
            "null_checks": null_checks
        },
        "performance": {
            "records_per_second": )" << args.records << R" / duration if duration > 0 else 0
        }
    }
    
    # Save ETL report
    with open(output_path + "/etl_report.json", "w") as f:
        json.dump(etl_report, f, indent=2, default=str)
    
    print("\n📊 ETL PIPELINE SUMMARY")
    print("=" * 50)
    print(f"Duration: {duration:.2f} seconds")
    print(f"Input Records: {)" << args.records << R"(}")
    print(f"Output Records: {etl_report['data_volumes']['total_output_records']}")
    print(f"Quality Score: {quality_score:.1f}%")
    print(f"Processing Rate: {etl_report['performance']['records_per_second']:.0f} records/second")
    print(f"Output Path: {output_path}")
    print("=" * 50)
    
    spark.stop()

if __name__ == "__main__":
    main()
)";
    
    script.close();
    
    // Make script executable
    std::string chmodCmd = "chmod +x " + scriptPath;
    std::system(chmodCmd.c_str());
    
    LOG_INFO("✅ Created ETL pipeline script: " + scriptPath);
}

/**
 * Main ETL workflow.
 */
int main(int argc, char* argv[]) {
    try {
        // Parse command line arguments
        CommandLineArgs args = parseCommandLineArgs(argc, argv);
        
        if (args.help) {
            printUsage(argv[0]);
            return 0;
        }
        
        LOG_INFO("🚀 Starting C++ Spark ETL Pipeline");
        LOG_INFO("   Records to process: " + std::to_string(args.records));
        LOG_INFO("   Output path: " + args.outputPath);
        LOG_INFO("   Spark master: " + args.master);
        
        PerformanceMetrics totalMetrics("Total ETL Pipeline");
        
        // Execute ETL stages
        generateETLSources(args);
        createETLScript(args);
        
        SparkConfig config = createSparkConfig(args);
        config.appName = args.appName + "_ETL_Pipeline";
        std::string scriptPath = args.outputPath + "/spark_etl_pipeline.py";
        
        if (!checkSparkCluster(config.master)) {
            LOG_WARN("Spark cluster not accessible, using local mode");
            config.master = "local[*]";
        }
        
        LOG_INFO("🔄 LOAD: Executing Spark ETL pipeline...");
        int result = executeSparkJob(config, scriptPath);
        
        if (result != 0) {
            throw SparkException("Spark ETL job failed with exit code: " + std::to_string(result));
        }
        
        totalMetrics.finish(args.records);
        
        // Print comprehensive summary
        std::cout << "\n📊 C++ SPARK ETL PIPELINE SUMMARY\n";
        std::cout << "==================================\n";
        
        std::cout << "\n⏱️  EXECUTION METRICS:\n";
        std::cout << "   Total Duration: " << std::fixed << std::setprecision(2) 
                  << totalMetrics.getDurationSeconds() << " seconds\n";
        std::cout << "   Records Processed: " << args.records << "\n";
        std::cout << "   Processing Rate: " << std::fixed << std::setprecision(0) 
                  << totalMetrics.getRecordsPerSecond() << " records/second\n";
        
        std::cout << "\n🔄 ETL STAGES COMPLETED:\n";
        std::cout << "   ✅ EXTRACT: Multi-source data generation (C++)\n";
        std::cout << "   ✅ TRANSFORM: Business logic and data cleaning (Spark)\n";
        std::cout << "   ✅ VALIDATE: Data quality checks and scoring (Spark)\n";
        std::cout << "   ✅ LOAD: Partitioned storage and reports (Spark)\n";
        
        std::cout << "\n📁 OUTPUT STRUCTURE:\n";
        std::cout << "   " << args.outputPath << "/\n";
        std::cout << "   ├── sources/           # Raw input data (CSV)\n";
        std::cout << "   ├── etl_output/\n";
        std::cout << "   │   ├── fact/          # Sales fact table (Parquet, partitioned)\n";
        std::cout << "   │   ├── dim/           # Customer & Product dimensions (Parquet)\n";
        std::cout << "   │   ├── agg/           # Aggregated tables (Parquet)\n";
        std::cout << "   │   ├── reports/       # Business reports (CSV)\n";
        std::cout << "   │   └── etl_report.json # Pipeline execution report\n";
        std::cout << "   └── spark_etl_pipeline.py # Generated Spark script\n";
        
        std::cout << "\n🔗 INTEGRATION HIGHLIGHTS:\n";
        std::cout << "   - C++ high-performance data generation\n";
        std::cout << "   - Spark distributed processing and analytics\n";
        std::cout << "   - Automated data quality validation\n";
        std::cout << "   - Multiple output formats (Parquet, CSV, JSON)\n";
        std::cout << "   - Partitioned storage for query performance\n";
        
        std::cout << "\n💡 NEXT STEPS:\n";
        std::cout << "   - Review ETL report: " << args.outputPath << "/etl_output/etl_report.json\n";
        std::cout << "   - Query fact/dim tables with Spark SQL or BI tools\n";
        std::cout << "   - Use business reports for stakeholder analysis\n";
        std::cout << "   - Monitor Spark UI: http://192.168.1.184:4040\n";
        
        std::cout << "\n==================================\n";
        
        LOG_INFO("✅ C++ Spark ETL pipeline completed successfully!");
        LOG_INFO("   Processed " + std::to_string(args.records) + " source records");
        LOG_INFO("   Results available in: " + args.outputPath);
        
        return 0;
        
    } catch (const SparkException& e) {
        LOG_ERROR("Spark error: " + std::string(e.what()));
        return 1;
    } catch (const std::exception& e) {
        LOG_ERROR("Error: " + std::string(e.what()));
        return 1;
    } catch (...) {
        LOG_ERROR("Unknown error occurred");
        return 1;
    }
}
