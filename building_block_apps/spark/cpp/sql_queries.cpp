#include "spark_common.h"

/**
 * Spark SQL Examples - Building Block Application (C++)
 * 
 * This application demonstrates C++ integration with Spark SQL:
 * - Data preparation and schema definition in C++
 * - Complex SQL query generation and execution
 * - Performance analysis and optimization
 * 
 * Usage:
 *   bazel run //spark/cpp:sql_queries -- --records 100000
 * 
 * Examples:
 *   # Basic SQL analytics
 *   bazel run //spark/cpp:sql_queries
 *   
 *   # Large dataset analysis
 *   bazel run //spark/cpp:sql_queries -- --records 1000000
 */

using namespace SparkCommon;

/**
 * Generate data and create SQL script.
 */
void createSQLScript(const CommandLineArgs& args) {
    LOG_INFO("ðŸ”„ Step 1: Creating Spark SQL script...");
    
    std::string scriptPath = args.outputPath + "/spark_sql_processor.py";
    std::ofstream script(scriptPath);
    
    if (!script.is_open()) {
        throw SparkException("Cannot create SQL script: " + scriptPath);
    }
    
    script << R"(#!/usr/bin/env python3
"""
Spark SQL Script (Generated by C++ Application)
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import sys

def main():
    # Create Spark session
    spark = SparkSession.builder \
        .appName(")" << args.appName << R"(_SQL") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()
    
    input_path = ")" << args.outputPath << R"(/input/sample_data.csv"
    output_path = ")" << args.outputPath << R"(/sql_output"
    
    print("ðŸ”„ Loading data for SQL analysis...")
    
    # Load data
    df = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .csv(input_path)
    
    # Create temporary view
    df.createOrReplaceTempView("sales")
    print(f"   Created 'sales' table with {df.count()} records")
    
    # Show schema
    print("\nðŸ“‹ Table Schema:")
    spark.sql("DESCRIBE sales").show()
    
    print("\nðŸ”„ Executing SQL queries...")
    
    # 1. Basic sales summary by category
    print("   1. Sales summary by category...")
    sales_summary = spark.sql("""
        SELECT 
            category,
            COUNT(*) as order_count,
            SUM(price * quantity) as total_revenue,
            AVG(price * quantity) as avg_order_value,
            MIN(price * quantity) as min_order_value,
            MAX(price * quantity) as max_order_value
        FROM sales
        WHERE price > 0 AND quantity > 0
        GROUP BY category
        ORDER BY total_revenue DESC
    """)
    
    print("   Sales Summary:")
    sales_summary.show()
    
    # 2. Daily trends
    print("   2. Daily sales trends...")
    daily_trends = spark.sql("""
        SELECT 
            DATE(timestamp) as sale_date,
            COUNT(*) as daily_orders,
            SUM(price * quantity) as daily_revenue,
            COUNT(DISTINCT user_id) as unique_customers,
            AVG(price * quantity) as avg_order_value
        FROM sales
        GROUP BY DATE(timestamp)
        ORDER BY sale_date DESC
        LIMIT 15
    """)
    
    print("   Daily Trends:")
    daily_trends.show()
    
    # 3. User behavior with window functions
    print("   3. User behavior analysis...")
    user_behavior = spark.sql("""
        SELECT 
            user_id,
            COUNT(*) as total_orders,
            SUM(price * quantity) as total_spent,
            AVG(price * quantity) as avg_order_value,
            ROW_NUMBER() OVER (ORDER BY SUM(price * quantity) DESC) as spending_rank,
            CASE 
                WHEN SUM(price * quantity) >= 1000 THEN 'High Value'
                WHEN SUM(price * quantity) >= 500 THEN 'Medium Value'
                ELSE 'Low Value'
            END as customer_tier
        FROM sales
        GROUP BY user_id
        ORDER BY total_spent DESC
        LIMIT 20
    """)
    
    print("   Top Users by Spending:")
    user_behavior.show()
    
    # 4. Advanced analytics with CTEs
    print("   4. Advanced analytics with CTEs...")
    advanced_analytics = spark.sql("""
        WITH hourly_stats AS (
            SELECT 
                HOUR(timestamp) as hour,
                category,
                COUNT(*) as transactions,
                SUM(price * quantity) as revenue
            FROM sales
            GROUP BY HOUR(timestamp), category
        ),
        peak_hours AS (
            SELECT 
                hour,
                SUM(transactions) as total_transactions,
                SUM(revenue) as total_revenue
            FROM hourly_stats
            GROUP BY hour
        )
        SELECT 
            hour,
            total_transactions,
            total_revenue,
            RANK() OVER (ORDER BY total_revenue DESC) as revenue_rank
        FROM peak_hours
        ORDER BY total_revenue DESC
        LIMIT 10
    """)
    
    print("   Peak Hours Analysis:")
    advanced_analytics.show()
    
    # 5. Statistical analysis
    print("   5. Statistical analysis...")
    stats_analysis = spark.sql("""
        SELECT 
            category,
            COUNT(*) as sample_size,
            AVG(price * quantity) as mean_value,
            STDDEV(price * quantity) as std_deviation,
            PERCENTILE_APPROX(price * quantity, 0.5) as median,
            PERCENTILE_APPROX(price * quantity, 0.9) as p90,
            PERCENTILE_APPROX(price * quantity, 0.95) as p95
        FROM sales
        GROUP BY category
        ORDER BY mean_value DESC
    """)
    
    print("   Statistical Analysis:")
    stats_analysis.show()
    
    print("ðŸ”„ Saving SQL results...")
    
    # Save results
    sales_summary.coalesce(1) \
        .write \
        .mode("overwrite") \
        .option("header", "true") \
        .csv(output_path + "/sales_summary")
    
    daily_trends.coalesce(1) \
        .write \
        .mode("overwrite") \
        .option("header", "true") \
        .csv(output_path + "/daily_trends")
    
    user_behavior.coalesce(1) \
        .write \
        .mode("overwrite") \
        .option("header", "true") \
        .csv(output_path + "/user_behavior")
    
    advanced_analytics.coalesce(1) \
        .write \
        .mode("overwrite") \
        .option("header", "true") \
        .csv(output_path + "/peak_hours")
    
    stats_analysis.coalesce(1) \
        .write \
        .mode("overwrite") \
        .option("header", "true") \
        .csv(output_path + "/statistical_analysis")
    
    print("âœ… SQL processing completed")
    print(f"   Results saved to: {output_path}")
    
    # Print overall summary
    overall_stats = spark.sql("""
        SELECT 
            COUNT(*) as total_records,
            COUNT(DISTINCT user_id) as unique_users,
            COUNT(DISTINCT category) as categories,
            SUM(price * quantity) as total_revenue
        FROM sales
    """).collect()[0]
    
    print(f"\nðŸ“Š OVERALL STATISTICS:")
    print(f"   Total Records: {overall_stats['total_records']}")
    print(f"   Unique Users: {overall_stats['unique_users']}")
    print(f"   Categories: {overall_stats['categories']}")
    print(f"   Total Revenue: ${overall_stats['total_revenue']:.2f}")
    
    spark.stop()

if __name__ == "__main__":
    main()
)";
    
    script.close();
    
    // Make script executable
    std::string chmodCmd = "chmod +x " + scriptPath;
    std::system(chmodCmd.c_str());
    
    LOG_INFO("âœ… Created Spark SQL script: " + scriptPath);
}

/**
 * Main SQL workflow.
 */
int main(int argc, char* argv[]) {
    try {
        // Parse command line arguments
        CommandLineArgs args = parseCommandLineArgs(argc, argv);
        
        if (args.help) {
            printUsage(argv[0]);
            return 0;
        }
        
        LOG_INFO("ðŸš€ Starting C++ Spark SQL Example");
        LOG_INFO("   Records to process: " + std::to_string(args.records));
        LOG_INFO("   Output path: " + args.outputPath);
        LOG_INFO("   Spark master: " + args.master);
        
        PerformanceMetrics totalMetrics("Total SQL Processing");
        
        // Step 1: Generate sample data
        LOG_INFO("ðŸ”„ Step 1: Generating sample data...");
        auto data = generateSampleData(args.records);
        
        createDirectory(args.outputPath);
        createDirectory(args.outputPath + "/input");
        
        std::string csvFile = args.outputPath + "/input/sample_data.csv";
        saveSampleDataToCSV(data, csvFile);
        
        // Step 2: Create and execute SQL script
        createSQLScript(args);
        
        SparkConfig config = createSparkConfig(args);
        config.appName = args.appName + "_SQL";
        std::string scriptPath = args.outputPath + "/spark_sql_processor.py";
        
        if (!checkSparkCluster(config.master)) {
            LOG_WARN("Spark cluster not accessible, using local mode");
            config.master = "local[*]";
        }
        
        LOG_INFO("ðŸ”„ Step 3: Executing Spark SQL...");
        int result = executeSparkJob(config, scriptPath);
        
        if (result != 0) {
            throw SparkException("Spark SQL job failed with exit code: " + std::to_string(result));
        }
        
        totalMetrics.finish(args.records);
        
        // Print summary
        std::cout << "\nðŸ“Š SPARK SQL RESULTS SUMMARY\n";
        std::cout << "=============================\n";
        
        std::cout << "\nðŸ“ˆ PROCESSING METRICS:\n";
        std::cout << "   Records Processed: " << args.records << "\n";
        std::cout << "   Processing Time: " << std::fixed << std::setprecision(2) 
                  << totalMetrics.getDurationSeconds() << " seconds\n";
        std::cout << "   Records per Second: " << std::fixed << std::setprecision(0) 
                  << totalMetrics.getRecordsPerSecond() << "\n";
        
        std::cout << "\nðŸ“ OUTPUT FILES:\n";
        std::vector<std::string> outputs = {
            "sales_summary", "daily_trends", "user_behavior", 
            "peak_hours", "statistical_analysis"
        };
        
        for (const auto& output : outputs) {
            std::cout << "   âœ… " << args.outputPath << "/sql_output/" << output << "\n";
        }
        
        std::cout << "\nðŸ”— SQL CAPABILITIES DEMONSTRATED:\n";
        std::cout << "   - Basic aggregations (GROUP BY, SUM, COUNT, AVG)\n";
        std::cout << "   - Window functions (ROW_NUMBER, RANK, OVER)\n";
        std::cout << "   - Common Table Expressions (CTEs)\n";
        std::cout << "   - Statistical functions (PERCENTILE_APPROX, STDDEV)\n";
        std::cout << "   - Date/time functions (DATE, HOUR)\n";
        std::cout << "   - Conditional logic (CASE WHEN)\n";
        
        std::cout << "\nðŸ’¡ PERFORMANCE OPTIMIZATIONS:\n";
        std::cout << "   - Adaptive Query Execution enabled\n";
        std::cout << "   - Partition coalescing for output files\n";
        std::cout << "   - Efficient CSV output with headers\n";
        
        std::cout << "\n=============================\n";
        
        LOG_INFO("âœ… C++ Spark SQL example completed successfully!");
        LOG_INFO("   Processed " + std::to_string(args.records) + " records");
        LOG_INFO("   Results available in: " + args.outputPath + "/sql_output");
        
        return 0;
        
    } catch (const SparkException& e) {
        LOG_ERROR("Spark error: " + std::string(e.what()));
        return 1;
    } catch (const std::exception& e) {
        LOG_ERROR("Error: " + std::string(e.what()));
        return 1;
    } catch (...) {
        LOG_ERROR("Unknown error occurred");
        return 1;
    }
}
