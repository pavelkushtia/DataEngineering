#include "spark_common.h"

/**
 * Spark Streaming Examples - Building Block Application (C++)
 * 
 * This application demonstrates C++ integration with Spark streaming:
 * - Real-time data generation and publishing to Kafka
 * - Integration with Spark Structured Streaming
 * - Performance monitoring for streaming workloads
 * 
 * Usage:
 *   bazel run //spark/cpp:streaming -- --kafka-topic events --duration 120
 * 
 * Examples:
 *   # Basic streaming
 *   bazel run //spark/cpp:streaming
 *   
 *   # Custom topic and duration
 *   bazel run //spark/cpp:streaming -- --kafka-topic user-events --duration 300
 */

using namespace SparkCommon;

/**
 * Generate streaming data to Kafka topic.
 */
void generateStreamingData(const CommandLineArgs& args) {
    LOG_INFO("üîÑ Step 1: Setting up streaming data generation...");
    
    // Create sample data for streaming
    auto sampleData = generateSampleData(1000); // Generate sample patterns
    
    createDirectory(args.outputPath);
    createDirectory(args.outputPath + "/streaming");
    
    // Save sample message format
    std::string sampleFile = args.outputPath + "/streaming/sample_messages.json";
    saveSampleDataToJSON(sampleData, sampleFile);
    
    LOG_INFO("üìù Sample Kafka message format saved to: " + sampleFile);
    LOG_INFO("   To test streaming, send JSON messages to Kafka topic: " + args.kafkaTopic);
    LOG_INFO("   Example command:");
    LOG_INFO("   kafka-console-producer.sh --topic " + args.kafkaTopic + " --bootstrap-server 192.168.1.184:9092 < " + sampleFile);
}

/**
 * Create Spark Streaming script.
 */
void createStreamingScript(const CommandLineArgs& args) {
    LOG_INFO("üîÑ Step 2: Creating Spark Streaming script...");
    
    std::string scriptPath = args.outputPath + "/spark_streaming_processor.py";
    std::ofstream script(scriptPath);
    
    if (!script.is_open()) {
        throw SparkException("Cannot create streaming script: " + scriptPath);
    }
    
    script << R"(#!/usr/bin/env python3
"""
Spark Structured Streaming Script (Generated by C++ Application)
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import sys

def main():
    # Create Spark session with streaming configuration
    spark = SparkSession.builder \
        .appName(")" << args.appName << R"(_Streaming") \
        .config("spark.sql.streaming.metricsEnabled", "true") \
        .getOrCreate()
    
    kafka_servers = "192.168.1.184:9092,192.168.1.187:9092,192.168.1.190:9092"
    kafka_topic = ")" << args.kafkaTopic << R"("
    output_path = ")" << args.outputPath << R"(/streaming/output"
    duration = )" << args.duration << R"(
    
    print(f"üîÑ Setting up Kafka stream from topic: {kafka_topic}")
    print(f"   Kafka servers: {kafka_servers}")
    print(f"   Duration: {duration} seconds")
    
    # Define schema for Kafka messages
    schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("product_id", StringType(), True),
        StructField("category", StringType(), True),
        StructField("action", StringType(), True),
        StructField("timestamp", StringType(), True),
        StructField("price", DoubleType(), True),
        StructField("quantity", IntegerType(), True),
        StructField("session_id", StringType(), True),
        StructField("is_premium", BooleanType(), True),
        StructField("region", StringType(), True)
    ])
    
    # Read from Kafka
    kafka_df = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", kafka_servers) \
        .option("subscribe", kafka_topic) \
        .option("startingOffsets", "latest") \
        .option("failOnDataLoss", "false") \
        .load()
    
    # Parse JSON messages
    parsed_df = kafka_df \
        .select(from_json(col("value").cast("string"), schema).alias("data")) \
        .select("data.*") \
        .withColumn("processed_timestamp", current_timestamp()) \
        .withColumn("event_timestamp", to_timestamp(col("timestamp"), "yyyy-MM-dd HH:mm:ss")) \
        .withColumn("total_amount", col("price") * col("quantity")) \
        .filter(col("event_timestamp").isNotNull())
    
    print("‚úÖ Kafka stream created and parsed")
    
    # Real-time analytics
    print("üîÑ Setting up real-time analytics...")
    
    # 1. Event counts per minute
    minute_counts = parsed_df \
        .withWatermark("event_timestamp", "2 minutes") \
        .groupBy(
            window(col("event_timestamp"), "1 minute"),
            col("action")
        ) \
        .agg(
            count("*").alias("event_count"),
            sum("total_amount").alias("total_revenue")
        )
    
    # 2. High-value alerts
    high_value_alerts = parsed_df \
        .filter(col("total_amount") > 500) \
        .select(
            col("user_id"),
            col("total_amount"),
            col("category"),
            col("event_timestamp"),
            lit("HIGH_VALUE_TRANSACTION").alias("alert_type")
        )
    
    print("üîÑ Starting streaming queries...")
    
    # Console output for monitoring
    console_query1 = minute_counts \
        .writeStream \
        .outputMode("update") \
        .format("console") \
        .option("truncate", "false") \
        .option("numRows", 10) \
        .trigger(processingTime="30 seconds") \
        .start()
    
    console_query2 = high_value_alerts \
        .writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", "false") \
        .trigger(processingTime="10 seconds") \
        .start()
    
    # File output
    file_query = minute_counts \
        .writeStream \
        .outputMode("update") \
        .format("parquet") \
        .option("path", output_path + "/minute_counts") \
        .option("checkpointLocation", output_path + "/checkpoints/minute_counts") \
        .trigger(processingTime="1 minute") \
        .start()
    
    print(f"‚úÖ {3} streaming queries started")
    
    # Monitor for specified duration
    import time
    start_time = time.time()
    
    try:
        while time.time() - start_time < duration:
            elapsed = int(time.time() - start_time)
            remaining = duration - elapsed
            print(f"\n‚è±Ô∏è  Elapsed: {elapsed}s | Remaining: {remaining}s")
            
            # Check query status
            queries = [console_query1, console_query2, file_query]
            active_count = sum(1 for q in queries if q.isActive)
            print(f"   Active Queries: {active_count}/{len(queries)}")
            
            time.sleep(15)
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Streaming interrupted by user")
    
    # Stop queries
    print("\nüîÑ Stopping streaming queries...")
    for query in [console_query1, console_query2, file_query]:
        if query.isActive:
            query.stop()
    
    print("‚úÖ All streaming queries stopped")
    print(f"   Results available in: {output_path}")
    
    spark.stop()

if __name__ == "__main__":
    main()
)";
    
    script.close();
    
    // Make script executable
    std::string chmodCmd = "chmod +x " + scriptPath;
    std::system(chmodCmd.c_str());
    
    LOG_INFO("‚úÖ Created Spark Streaming script: " + scriptPath);
}

/**
 * Execute Spark Streaming job.
 */
void executeStreamingJob(const CommandLineArgs& args) {
    LOG_INFO("üîÑ Step 3: Executing Spark Streaming...");
    
    SparkConfig config = createSparkConfig(args);
    config.appName = args.appName + "_Streaming";
    std::string scriptPath = args.outputPath + "/spark_streaming_processor.py";
    
    // Check Spark cluster connectivity
    if (!checkSparkCluster(config.master)) {
        LOG_WARN("Spark cluster not accessible, using local mode");
        config.master = "local[*]";
    }
    
    LOG_INFO("Starting streaming job for " + std::to_string(args.duration) + " seconds...");
    
    // Execute Spark streaming job
    int result = executeSparkJob(config, scriptPath);
    
    if (result != 0) {
        throw SparkException("Spark streaming job failed with exit code: " + std::to_string(result));
    }
    
    LOG_INFO("‚úÖ Spark streaming completed successfully");
}

/**
 * Print streaming summary.
 */
void printStreamingSummary(const CommandLineArgs& args) {
    std::cout << "\nüìä SPARK STREAMING RESULTS SUMMARY\n";
    std::cout << "===================================\n";
    
    std::cout << "\n‚è±Ô∏è  STREAMING METRICS:\n";
    std::cout << "   Duration: " << args.duration << " seconds\n";
    std::cout << "   Kafka Topic: " << args.kafkaTopic << "\n";
    std::cout << "   Output Path: " << args.outputPath << "/streaming/output\n";
    
    std::cout << "\nüîó INTEGRATION WORKFLOW:\n";
    std::cout << "   1. C++ generates sample message formats\n";
    std::cout << "   2. C++ creates optimized Spark Streaming script\n";
    std::cout << "   3. Spark processes real-time Kafka streams\n";
    std::cout << "   4. Results saved to Parquet files\n";
    
    std::cout << "\nüìù TO TEST STREAMING:\n";
    std::cout << "   Send messages to Kafka topic '" << args.kafkaTopic << "':\n";
    std::cout << "   \n";
    std::cout << "   kafka-console-producer.sh \\\n";
    std::cout << "     --topic " << args.kafkaTopic << " \\\n";
    std::cout << "     --bootstrap-server 192.168.1.184:9092\n";
    std::cout << "   \n";
    std::cout << "   Sample message format available in:\n";
    std::cout << "   " << args.outputPath << "/streaming/sample_messages.json\n";
    
    std::cout << "\nüí° MONITORING:\n";
    std::cout << "   - Spark Streaming UI: http://192.168.1.184:4040\n";
    std::cout << "   - Real-time metrics displayed in console\n";
    std::cout << "   - High-value transaction alerts\n";
    
    std::cout << "\n===================================\n";
}

/**
 * Main streaming workflow.
 */
int main(int argc, char* argv[]) {
    try {
        // Parse command line arguments
        CommandLineArgs args = parseCommandLineArgs(argc, argv);
        
        if (args.help) {
            printUsage(argv[0]);
            return 0;
        }
        
        LOG_INFO("üöÄ Starting C++ Spark Streaming Example");
        LOG_INFO("   Kafka topic: " + args.kafkaTopic);
        LOG_INFO("   Duration: " + std::to_string(args.duration) + " seconds");
        LOG_INFO("   Output path: " + args.outputPath);
        LOG_INFO("   Spark master: " + args.master);
        
        PerformanceMetrics totalMetrics("Total Streaming Setup");
        
        // Execute streaming pipeline
        generateStreamingData(args);
        createStreamingScript(args);
        executeStreamingJob(args);
        
        totalMetrics.finish();
        
        // Print summary
        printStreamingSummary(args);
        
        LOG_INFO("‚úÖ C++ Spark streaming example completed!");
        LOG_INFO("   Streaming script executed for " + std::to_string(args.duration) + " seconds");
        LOG_INFO("   Results available in: " + args.outputPath + "/streaming");
        
        return 0;
        
    } catch (const SparkException& e) {
        LOG_ERROR("Spark error: " + std::string(e.what()));
        return 1;
    } catch (const std::exception& e) {
        LOG_ERROR("Error: " + std::string(e.what()));
        return 1;
    } catch (...) {
        LOG_ERROR("Unknown error occurred");
        return 1;
    }
}
