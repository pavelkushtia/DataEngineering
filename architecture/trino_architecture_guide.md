# Trino Distributed Architecture Guide

## ğŸ—ï¸ Your Current 3-Node Trino Setup

This guide explains the Trino architecture based on **your exact setup**:
- **cpu-node1** (192.168.1.184) - Trino Coordinator + Web UI
- **cpu-node2** (192.168.1.187) - Trino Worker 1  
- **worker-node3** (192.168.1.190) - Trino Worker 2

---

## ğŸ“š Table of Contents

1. [What is Trino? (Simple Explanation)](#what-is-trino-simple-explanation)
2. [Your Current Architecture](#your-current-architecture)
3. [Coordinator Architecture](#coordinator-architecture)
4. [Worker Architecture](#worker-architecture)
5. [Query Execution Flow](#query-execution-flow)
6. [Memory Architecture](#memory-architecture)
7. [Connectors Architecture](#connectors-architecture)
8. [Discovery & Communication](#discovery--communication)
9. [Query Planning & Optimization](#query-planning--optimization)
10. [Distributed Joins & Aggregations](#distributed-joins--aggregations)
11. [Security Architecture](#security-architecture)
12. [Scaling Your Setup](#scaling-your-setup)
13. [Performance Tuning](#performance-tuning)
14. [Architecture Changes When Adding Nodes](#architecture-changes-when-adding-nodes)

---

## ğŸ¤” What is Trino? (Simple Explanation)

**Think of Trino like a distributed data query orchestra:**

- **Coordinator** = Orchestra Conductor (plans the music, coordinates everything)
- **Workers** = Orchestra Musicians (execute their assigned parts)
- **Connectors** = Musical Instruments (interface with different data sources)
- **Catalogs** = Sheet Music (metadata about where data lives)
- **Splits** = Musical Measures (chunks of data to process)
- **Stages** = Orchestra Sections (groups of related tasks)
- **Exchanges** = Musical Handoffs (data flowing between stages)

**Why distributed?** Just like an orchestra can play complex symphonies by having specialized musicians, Trino can query massive datasets across different systems by coordinating specialized workers.

**Key Difference from Spark:** Trino is **SQL-only** and **interactive** - built for fast analytics, not batch processing or machine learning.

---

## ğŸ›ï¸ Your Current Architecture

### Overall System View

**What you have:** A 3-node distributed Trino cluster in Coordinator-Worker architecture.

### **Plain English Explanation:**
- **1 Coordinator** - The "conductor" that plans queries and coordinates execution
- **2 Workers** - The "performers" that actually read data and execute query logic
- **HTTP Communication** - All coordination happens via REST APIs (no SSH needed)
- **Multiple Catalogs** - Can query PostgreSQL, Kafka, memory, and more simultaneously

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Your Trino Cluster                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    cpu-node1        â”‚    cpu-node2        â”‚   worker-node3          â”‚
â”‚   (Coordinator)     â”‚    (Worker 1)       â”‚    (Worker 2)           â”‚
â”‚                     â”‚                     â”‚                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Trino         â”‚  â”‚  â”‚ Trino Worker  â”‚  â”‚  â”‚  Trino Worker     â”‚  â”‚
â”‚  â”‚ Coordinator   â”‚  â”‚  â”‚ - Task Exec   â”‚  â”‚  â”‚  - Task Exec      â”‚  â”‚
â”‚  â”‚ - Query Plan  â”‚  â”‚  â”‚ - Data Proc   â”‚  â”‚  â”‚  - Data Proc      â”‚  â”‚
â”‚  â”‚ - Resource    â”‚  â”‚  â”‚ - Connectors  â”‚  â”‚  â”‚  - Connectors     â”‚  â”‚
â”‚  â”‚   Scheduling  â”‚  â”‚  â”‚ - Memory Mgmt â”‚  â”‚  â”‚  - Memory Mgmt    â”‚  â”‚
â”‚  â”‚ - Web UI:8081 â”‚  â”‚  â”‚ - Exchange    â”‚  â”‚  â”‚  - Exchange       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                     â”‚                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Catalogs    â”‚  â”‚  â”‚   Catalogs    â”‚  â”‚  â”‚    Catalogs       â”‚  â”‚
â”‚  â”‚ - PostgreSQL  â”‚  â”‚  â”‚ - PostgreSQL  â”‚  â”‚  â”‚  - PostgreSQL     â”‚  â”‚
â”‚  â”‚ - Kafka       â”‚  â”‚  â”‚ - Kafka       â”‚  â”‚  â”‚  - Kafka          â”‚  â”‚
â”‚  â”‚ - Iceberg     â”‚  â”‚  â”‚ - Iceberg     â”‚  â”‚  â”‚  - Iceberg        â”‚  â”‚
â”‚  â”‚ - Delta Lake  â”‚  â”‚  â”‚ - Delta Lake  â”‚  â”‚  â”‚  - Delta Lake     â”‚  â”‚
â”‚  â”‚ - Memory      â”‚  â”‚  â”‚ - Memory      â”‚  â”‚  â”‚  - Memory         â”‚  â”‚
â”‚  â”‚ - TPC-H       â”‚  â”‚  â”‚ - TPC-H       â”‚  â”‚  â”‚  - TPC-H          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                     â”‚                         â”‚
â”‚  192.168.1.184      â”‚  192.168.1.187      â”‚  192.168.1.190          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Network Communication Ports:**
- **8081**: HTTP API & Web UI (primary communication)
- **6123**: Discovery service (optional clustering)
- **8443**: HTTPS (if SSL enabled)

---

## ğŸ¯ Coordinator Architecture

### **Role: The Query Orchestrator**

The Trino Coordinator is the **query brain** that plans, schedules, and monitors all queries but doesn't process data itself.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Trino Coordinator (cpu-node1)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Web UI & API  â”‚    â”‚  Query Planner  â”‚    â”‚  Resource Mgr   â”‚  â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚  â”‚
â”‚  â”‚ â€¢ REST API      â”‚    â”‚ â€¢ SQL Parsing   â”‚    â”‚ â€¢ Memory Pools  â”‚  â”‚
â”‚  â”‚ â€¢ Web Interface â”‚    â”‚ â€¢ Optimization  â”‚    â”‚ â€¢ CPU Allocationâ”‚  â”‚
â”‚  â”‚ â€¢ Authenticationâ”‚    â”‚ â€¢ Cost Analysis â”‚    â”‚ â€¢ Node Discoveryâ”‚  â”‚
â”‚  â”‚ â€¢ Session Mgmt  â”‚    â”‚ â€¢ Join Strategy â”‚    â”‚ â€¢ Load Balancingâ”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                       â”‚                       â”‚        â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                   â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Query Execution â”‚    â”‚ Metadata Cache  â”‚    â”‚  Fault Toleranceâ”‚  â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚  â”‚
â”‚  â”‚ â€¢ Stage Creationâ”‚    â”‚ â€¢ Table Schemas â”‚    â”‚ â€¢ Query Retry   â”‚  â”‚
â”‚  â”‚ â€¢ Task Assign   â”‚    â”‚ â€¢ Partition Infoâ”‚    â”‚ â€¢ Node Failure  â”‚  â”‚
â”‚  â”‚ â€¢ Progress Trackâ”‚    â”‚ â€¢ Statistics    â”‚    â”‚ â€¢ Graceful Stop â”‚  â”‚
â”‚  â”‚ â€¢ Result Collectâ”‚    â”‚ â€¢ Catalog Cache â”‚    â”‚ â€¢ Health Checks â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Key Coordinator Components:**

#### **1. Query Parser & Planner**
- **SQL Analysis**: Parses SQL into Abstract Syntax Tree (AST)
- **Semantic Analysis**: Validates tables, columns, types
- **Query Optimization**: Cost-based optimization (CBO)
- **Execution Planning**: Creates distributed execution plan

#### **2. Resource Manager**
- **Memory Management**: Tracks memory usage per query
- **CPU Scheduling**: Distributes tasks across workers
- **Admission Control**: Prevents cluster overload
- **Node Health**: Monitors worker availability

#### **3. Metadata Manager**
- **Catalog Integration**: Connects to multiple data sources
- **Schema Caching**: Caches table definitions for performance
- **Statistics**: Maintains table/column statistics for optimization
- **Security**: Enforces access controls

---

## âš™ï¸ Worker Architecture

### **Role: The Data Processing Engines**

Trino Workers are the **workhorses** that actually read data from sources and execute query logic.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Trino Worker (cpu-node2/worker-node3)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Task Executor  â”‚    â”‚  Memory Manager â”‚    â”‚   Data Exchange â”‚  â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚  â”‚
â”‚  â”‚ â€¢ Pipeline Exec â”‚    â”‚ â€¢ Memory Pools  â”‚    â”‚ â€¢ Shuffle Data  â”‚  â”‚
â”‚  â”‚ â€¢ Operator Tree â”‚    â”‚ â€¢ Spill to Disk â”‚    â”‚ â€¢ Network I/O   â”‚  â”‚
â”‚  â”‚ â€¢ Thread Mgmt   â”‚    â”‚ â€¢ GC Management â”‚    â”‚ â€¢ Compression   â”‚  â”‚
â”‚  â”‚ â€¢ Page Buffers  â”‚    â”‚ â€¢ Buffer Pools  â”‚    â”‚ â€¢ Serialization â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                       â”‚                       â”‚        â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                   â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Connectors    â”‚    â”‚  Local Storage  â”‚    â”‚   Monitoring    â”‚  â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚  â”‚
â”‚  â”‚ â€¢ PostgreSQL    â”‚    â”‚ â€¢ Temp Files    â”‚    â”‚ â€¢ Metrics       â”‚  â”‚
â”‚  â”‚ â€¢ Kafka         â”‚    â”‚ â€¢ Spill Space   â”‚    â”‚ â€¢ Health Checks â”‚  â”‚
â”‚  â”‚ â€¢ Iceberg       â”‚    â”‚ â€¢ Page Cache    â”‚    â”‚ â€¢ Resource Usageâ”‚  â”‚
â”‚  â”‚ â€¢ Delta Lake    â”‚    â”‚ â€¢ File Buffers  â”‚    â”‚ â€¢ Task Progress â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Key Worker Components:**

#### **1. Task Execution Engine**
- **Pipeline Processing**: Vectorized execution for performance
- **Operator Trees**: Implements joins, aggregations, filters
- **Parallelism**: Multiple threads per worker
- **Memory-Aware**: Tracks memory usage per operation

#### **2. Connector Framework**
- **Plugin Architecture**: Modular connector system
- **Data Source Abstraction**: Uniform interface to different systems
- **Predicate Pushdown**: Pushes filters to data sources
- **Projection Pushdown**: Only reads needed columns

#### **3. Exchange Service**
- **Data Shuffling**: Moves data between query stages
- **Network Optimization**: Compression and batching
- **Fault Tolerance**: Handles network failures
- **Backpressure**: Prevents memory overflow

---

## ğŸ”„ Query Execution Flow

### **From SQL to Results: The Complete Journey**

Here's how your query `SELECT * FROM postgresql.sales JOIN kafka.events` executes:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Query Execution Flow                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1ï¸âƒ£ SQL SUBMISSION (Client â†’ Coordinator)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP POST     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Client    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚   Coordinator   â”‚
   â”‚ (sql query) â”‚                  â”‚   (parse SQL)   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2ï¸âƒ£ QUERY PLANNING (Coordinator)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Step 1: Parse SQL â†’ AST (Abstract Syntax Tree)                â”‚
   â”‚  Step 2: Semantic Analysis (validate tables, columns)          â”‚
   â”‚  Step 3: Cost-Based Optimization (join order, algorithms)      â”‚
   â”‚  Step 4: Create Distributed Plan (stages, splits, tasks)       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3ï¸âƒ£ STAGE CREATION (Coordinator â†’ Workers)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Stage 1       â”‚                    â”‚   Stage 2       â”‚
   â”‚ (Scan postgres) â”‚                    â”‚ (Scan kafka)    â”‚
   â”‚                 â”‚                    â”‚                 â”‚
   â”‚ Worker 1: Split â”‚                    â”‚ Worker 1: Topic â”‚
   â”‚ Worker 2: Split â”‚                    â”‚ Worker 2: Topic â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Stage 3       â”‚
                     â”‚  (Join + Send)  â”‚
                     â”‚                 â”‚
                     â”‚ Worker 1: Join  â”‚
                     â”‚ Worker 2: Join  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4ï¸âƒ£ TASK EXECUTION (Workers)
   Worker 1:                           Worker 2:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ â–¸ Read PostgreSQL   â”‚              â”‚ â–¸ Read Kafka        â”‚
   â”‚ â–¸ Apply Filters     â”‚              â”‚ â–¸ Parse JSON        â”‚
   â”‚ â–¸ Send via Exchange â”‚              â”‚ â–¸ Send via Exchange â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                                  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Join Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ â–¸ Hash Join         â”‚
                    â”‚ â–¸ Send Results      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

5ï¸âƒ£ RESULT COLLECTION (Coordinator)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ â–¸ Collect data from all workers                                 â”‚
   â”‚ â–¸ Apply final ordering/limiting                                 â”‚
   â”‚ â–¸ Stream results back to client                                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Stage Types:**

#### **Source Stages**
- **Purpose**: Read data from connectors
- **Parallelism**: Based on data splits (files, partitions, etc.)
- **Location**: Run on workers with best data locality

#### **Shuffle Stages**
- **Purpose**: Redistribute data for joins/aggregations
- **Parallelism**: Based on hash functions or ranges
- **Network**: Heavy data exchange between workers

#### **Final Stages**
- **Purpose**: Collect and return results
- **Parallelism**: Usually single-threaded on coordinator
- **Output**: Stream results to client

---

## ğŸ§  Memory Architecture

### **Memory Management Across the Cluster**

Trino uses sophisticated memory management to handle large queries efficiently:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Memory Architecture                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  COORDINATOR MEMORY (2GB)          WORKER MEMORY (4GB each)        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Query Management      â”‚      â”‚      Query Execution       â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚ â”‚ Query Metadata      â”‚ â”‚      â”‚ â”‚ Operator Memory         â”‚ â”‚   â”‚
â”‚  â”‚ â”‚ â€¢ Execution Plans   â”‚ â”‚      â”‚ â”‚ â€¢ Hash Tables          â”‚ â”‚   â”‚
â”‚  â”‚ â”‚ â€¢ Task Assignments  â”‚ â”‚      â”‚ â”‚ â€¢ Sort Buffers         â”‚ â”‚   â”‚
â”‚  â”‚ â”‚ â€¢ Progress Tracking â”‚ â”‚      â”‚ â”‚ â€¢ Aggregation Buffers  â”‚ â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â”‚                         â”‚      â”‚                             â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚ â”‚ Metadata Cache      â”‚ â”‚      â”‚ â”‚ Page Buffers            â”‚ â”‚   â”‚
â”‚  â”‚ â”‚ â€¢ Table Schemas     â”‚ â”‚      â”‚ â”‚ â€¢ Input Pages          â”‚ â”‚   â”‚
â”‚  â”‚ â”‚ â€¢ Column Stats      â”‚ â”‚      â”‚ â”‚ â€¢ Output Pages         â”‚ â”‚   â”‚
â”‚  â”‚ â”‚ â€¢ Partition Info    â”‚ â”‚      â”‚ â”‚ â€¢ Exchange Buffers     â”‚ â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚                             â”‚   â”‚
â”‚                                   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚                                   â”‚ â”‚ Connector Memory        â”‚ â”‚   â”‚
â”‚                                   â”‚ â”‚ â€¢ JDBC Buffers         â”‚ â”‚   â”‚
â”‚                                   â”‚ â”‚ â€¢ Kafka Consumers      â”‚ â”‚   â”‚
â”‚                                   â”‚ â”‚ â€¢ File System Cache   â”‚ â”‚   â”‚
â”‚                                   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Memory Limits (from your config):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ query.max-memory=8GB                (total across all workers)     â”‚
â”‚ query.max-memory-per-node=2GB       (per worker limit)             â”‚
â”‚ query.max-total-memory-per-node=3GB (per worker + overhead)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Memory Pool Management:**

#### **General Pool**
- **Purpose**: Normal query execution
- **Size**: Most of available memory
- **Usage**: Joins, aggregations, sorting

#### **Reserved Pool**
- **Purpose**: Large queries that exceed general pool
- **Size**: Small portion of memory
- **Usage**: Emergency allocation to prevent deadlocks

#### **System Pool**
- **Purpose**: System operations and metadata
- **Size**: Fixed small allocation
- **Usage**: Connector metadata, internal operations

### **Memory Pressure Handling:**

```
Low Memory â†’ Spill to Disk â†’ Kill Queries â†’ Reject New Queries
    â†“              â†“              â†“              â†“
 Normal Ops    Performance    Free Memory    Protect Cluster
```

---

## ğŸ”Œ Connectors Architecture

### **Multi-Source Query Capability**

Your Trino setup can query multiple data sources in a single SQL statement:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Connector Architecture                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚    COORDINATOR & WORKERS                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                     Trino SQL Engine                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                  â”‚                                  â”‚
â”‚                    Uniform SQL Interface                            â”‚
â”‚                                  â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                   Connector Framework                          â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ PostgreSQL  â”‚ â”‚   Kafka     â”‚ â”‚  Iceberg    â”‚ â”‚  Memory   â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ Connector   â”‚ â”‚ Connector   â”‚ â”‚ Connector   â”‚ â”‚ Connector â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ Delta Lake  â”‚ â”‚   TPC-H     â”‚ â”‚ File System â”‚ â”‚   Redis   â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ Connector   â”‚ â”‚ Connector   â”‚ â”‚ Connector   â”‚ â”‚ Connector â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                  â”‚                                  â”‚
â”‚               Data Source Protocols                                 â”‚
â”‚                                  â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚PostgreSQLâ”‚  â”‚  Kafka   â”‚  â”‚  HDFS/   â”‚  â”‚ In-Memoryâ”‚             â”‚
â”‚  â”‚Database  â”‚  â”‚ Clusters â”‚  â”‚   S3     â”‚  â”‚   Data   â”‚             â”‚
â”‚  â”‚          â”‚  â”‚          â”‚  â”‚          â”‚  â”‚          â”‚             â”‚
â”‚  â”‚Port: 5432â”‚  â”‚Port: 9092â”‚  â”‚ Files    â”‚  â”‚   RAM    â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **How Connectors Work:**

#### **1. Metadata Operations**
```sql
-- When you run: SHOW TABLES FROM postgresql.public;
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Coordinator asks PostgreSQL Connector:         â”‚
â”‚ "What tables exist in schema 'public'?"        â”‚
â”‚                                                 â”‚
â”‚ PostgreSQL Connector:                           â”‚
â”‚ â–¸ Connects to PostgreSQL (192.168.1.184:5432) â”‚
â”‚ â–¸ Runs: SELECT table_name FROM information_schema â”‚
â”‚ â–¸ Returns list to Coordinator                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **2. Data Splitting**
```sql
-- When you run: SELECT * FROM postgresql.sales;
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Coordinator asks PostgreSQL Connector:         â”‚
â”‚ "How should we split this table for parallel reading?" â”‚
â”‚                                                 â”‚
â”‚ PostgreSQL Connector:                           â”‚
â”‚ â–¸ Analyzes table size and structure             â”‚
â”‚ â–¸ Creates splits (e.g., by primary key ranges)  â”‚
â”‚ â–¸ Returns: [Split1: id 1-1000, Split2: id 1001-2000] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **3. Parallel Data Reading**
```sql
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker 1 (cpu-node2):                          â”‚
â”‚ â–¸ Gets Split1: "SELECT * FROM sales WHERE id BETWEEN 1 AND 1000" â”‚
â”‚ â–¸ Executes SQL against PostgreSQL               â”‚
â”‚ â–¸ Streams results to query engine               â”‚
â”‚                                                 â”‚
â”‚ Worker 2 (worker-node3):                       â”‚
â”‚ â–¸ Gets Split2: "SELECT * FROM sales WHERE id BETWEEN 1001 AND 2000" â”‚
â”‚ â–¸ Executes SQL against PostgreSQL               â”‚
â”‚ â–¸ Streams results to query engine               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Connector-Specific Optimizations:**

#### **PostgreSQL Connector**
- **Predicate Pushdown**: Filters pushed to database
- **Projection Pushdown**: Only selected columns retrieved
- **Connection Pooling**: Reuses database connections
- **Parallel Reads**: Multiple connections per worker

#### **Kafka Connector**
- **Partition Assignment**: Each split = Kafka partition
- **Offset Management**: Tracks message positions
- **Schema Registry**: Handles Avro/JSON schemas
- **Time-based Splits**: Can split by timestamp

#### **Iceberg Connector**
- **Snapshot Isolation**: Consistent table snapshots
- **Partition Pruning**: Skips irrelevant partitions
- **File-level Splits**: Each file becomes a split
- **Vectorized Reading**: High-performance columnar reads

---

## ğŸ” Discovery & Communication

### **How Nodes Find Each Other**

Unlike Spark/Flink, Trino uses **HTTP-based discovery** - no SSH required:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Discovery Process                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  STEP 1: Coordinator Starts                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ cpu-node1 (Coordinator)                                        â”‚ â”‚
â”‚  â”‚ â–¸ Starts discovery service on port 8081                        â”‚ â”‚
â”‚  â”‚ â–¸ discovery.uri=http://192.168.1.184:8081                     â”‚ â”‚
â”‚  â”‚ â–¸ Waits for workers to register                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                  â”‚                                  â”‚
â”‚                                  â–¼                                  â”‚
â”‚  STEP 2: Workers Auto-Register                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ cpu-node2 (Worker 1)     â”‚    â”‚ worker-node3 (Worker 2)      â”‚   â”‚
â”‚  â”‚                          â”‚    â”‚                              â”‚   â”‚
â”‚  â”‚ â–¸ Reads discovery.uri    â”‚    â”‚ â–¸ Reads discovery.uri        â”‚   â”‚
â”‚  â”‚ â–¸ HTTP POST to           â”‚    â”‚ â–¸ HTTP POST to               â”‚   â”‚
â”‚  â”‚   192.168.1.184:8081     â”‚    â”‚   192.168.1.184:8081         â”‚   â”‚
â”‚  â”‚ â–¸ Sends: node info       â”‚    â”‚ â–¸ Sends: node info           â”‚   â”‚
â”‚  â”‚   - IP address           â”‚    â”‚   - IP address               â”‚   â”‚
â”‚  â”‚   - Available memory     â”‚    â”‚   - Available memory         â”‚   â”‚
â”‚  â”‚   - CPU cores            â”‚    â”‚   - CPU cores                â”‚   â”‚
â”‚  â”‚   - Connectors           â”‚    â”‚   - Connectors               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                  â”‚                                  â”‚
â”‚                                  â–¼                                  â”‚
â”‚  STEP 3: Cluster Formation                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Coordinator maintains registry:                                 â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Active Nodes:                                                   â”‚ â”‚
â”‚  â”‚ â–¸ cpu-node1    (Coordinator) - 192.168.1.184:8081             â”‚ â”‚
â”‚  â”‚ â–¸ cpu-node2    (Worker)      - 192.168.1.187:8081             â”‚ â”‚
â”‚  â”‚ â–¸ worker-node3 (Worker)      - 192.168.1.190:8081             â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Total Resources:                                                â”‚ â”‚
â”‚  â”‚ â–¸ Memory: 6GB (2GB coord + 2GB worker1 + 2GB worker2)          â”‚ â”‚
â”‚  â”‚ â–¸ CPU: 6 cores total                                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Ongoing Communication:**

#### **Health Checks**
```
Every 30 seconds:
Workers â†’ Coordinator: "I'm alive and have X memory available"
Coordinator â†’ Workers: "Here are your new tasks"
```

#### **Task Assignment**
```
Query arrives:
Coordinator â†’ Worker 1: "Execute Stage 1, Split A"
Coordinator â†’ Worker 2: "Execute Stage 1, Split B"
Workers â†’ Coordinator: "Stage 1 complete, here's the data"
```

#### **No SSH Needed Because:**
- **Service Architecture**: Each node runs as systemd service
- **HTTP Communication**: All coordination via REST APIs
- **Self-Registration**: Workers find coordinator automatically
- **Graceful Handling**: Automatic retry and failover

---

## ğŸ§® Query Planning & Optimization

### **Cost-Based Optimization (CBO)**

Trino uses sophisticated optimization to make your cross-system queries fast:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Query Optimization Process                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  INPUT: SQL Query                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ SELECT c.name, SUM(o.amount)                                   â”‚ â”‚
â”‚  â”‚ FROM postgresql.customers c                                     â”‚ â”‚
â”‚  â”‚ JOIN kafka.orders o ON c.id = o.customer_id                    â”‚ â”‚
â”‚  â”‚ WHERE c.country = 'USA'                                        â”‚ â”‚
â”‚  â”‚ GROUP BY c.name                                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                  â”‚                                  â”‚
â”‚                                  â–¼                                  â”‚
â”‚  STEP 1: Parse & Validate                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â–¸ Parse SQL to Abstract Syntax Tree (AST)                      â”‚ â”‚
â”‚  â”‚ â–¸ Resolve table/column references across catalogs              â”‚ â”‚
â”‚  â”‚ â–¸ Type checking and validation                                  â”‚ â”‚
â”‚  â”‚ â–¸ Permission checks per connector                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                  â”‚                                  â”‚
â”‚                                  â–¼                                  â”‚
â”‚  STEP 2: Rule-Based Optimization                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â–¸ Predicate Pushdown:                                          â”‚ â”‚
â”‚  â”‚   - Push "country = 'USA'" to PostgreSQL                       â”‚ â”‚
â”‚  â”‚   - Reduce data movement                                        â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ â–¸ Projection Pushdown:                                         â”‚ â”‚
â”‚  â”‚   - Only read c.name, c.id from PostgreSQL                     â”‚ â”‚
â”‚  â”‚   - Only read o.customer_id, o.amount from Kafka               â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ â–¸ Join Reordering:                                             â”‚ â”‚
â”‚  â”‚   - Filter first, then join (smaller datasets)                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                  â”‚                                  â”‚
â”‚                                  â–¼                                  â”‚
â”‚  STEP 3: Cost-Based Optimization                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Statistics Collection:                                          â”‚ â”‚
â”‚  â”‚ â–¸ PostgreSQL: ANALYZE results (row counts, data distribution)  â”‚ â”‚
â”‚  â”‚ â–¸ Kafka: Topic partition info, message rates                   â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Join Algorithm Selection:                                       â”‚ â”‚
â”‚  â”‚ â–¸ Hash Join vs Broadcast Join vs Merge Join                    â”‚ â”‚
â”‚  â”‚ â–¸ Which table to build/probe                                   â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Parallelism Planning:                                           â”‚ â”‚
â”‚  â”‚ â–¸ How many splits per data source                              â”‚ â”‚
â”‚  â”‚ â–¸ Optimal number of workers per stage                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                  â”‚                                  â”‚
â”‚                                  â–¼                                  â”‚
â”‚  OUTPUT: Distributed Execution Plan                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Stage 0: [Source]                                              â”‚ â”‚
â”‚  â”‚ â–¸ Worker 1: Scan PostgreSQL customers WHERE country='USA'      â”‚ â”‚
â”‚  â”‚ â–¸ Worker 2: Scan PostgreSQL customers WHERE country='USA'      â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Stage 1: [Source]                                              â”‚ â”‚
â”‚  â”‚ â–¸ Worker 1: Scan Kafka orders (partitions 0,1)                â”‚ â”‚
â”‚  â”‚ â–¸ Worker 2: Scan Kafka orders (partitions 2,3)                â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Stage 2: [Join + Aggregate]                                    â”‚ â”‚
â”‚  â”‚ â–¸ Worker 1: Hash join + group by (hash(customer_id) % 2 = 0)   â”‚ â”‚
â”‚  â”‚ â–¸ Worker 2: Hash join + group by (hash(customer_id) % 2 = 1)   â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Stage 3: [Final Aggregate]                                     â”‚ â”‚
â”‚  â”‚ â–¸ Coordinator: Combine results from workers                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Optimization Techniques:**

#### **Cross-System Optimizations**
- **Data Locality**: Minimize network traffic between systems
- **Format Awareness**: Leverage Parquet column pruning, partition elimination
- **Parallel Reading**: Coordinate parallel reads across different systems
- **Join Location**: Decide where to perform joins (memory vs. network cost)

#### **Connector-Specific Optimizations**
```
PostgreSQL:
â–¸ SQL pushdown (WHERE, ORDER BY, LIMIT)
â–¸ Connection pooling
â–¸ Batch fetching

Kafka:
â–¸ Partition-aware parallelism
â–¸ Schema registry integration
â–¸ Consumer group management

Iceberg:
â–¸ Snapshot isolation
â–¸ Partition pruning
â–¸ File-level parallelism
```

---

## ğŸ¤ Distributed Joins & Aggregations

### **How Cross-System Joins Work**

The most complex part of distributed query processing - joining data from different systems:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Cross-System Join: PostgreSQL âŸµâŸ¶ Kafka                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  QUERY: SELECT * FROM postgresql.customers c                       â”‚
â”‚         JOIN kafka.orders o ON c.id = o.customer_id                â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STAGE 1: PARALLEL DATA EXTRACTION
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker 1 (cpu-node2)              Worker 2 (worker-node3)         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ PostgreSQL Scan         â”‚       â”‚ PostgreSQL Scan             â”‚  â”‚
â”‚  â”‚ â–¸ customers (id 1-5000) â”‚       â”‚ â–¸ customers (id 5001-10000) â”‚  â”‚
â”‚  â”‚ â–¸ Extract: id, name     â”‚       â”‚ â–¸ Extract: id, name         â”‚  â”‚
â”‚  â”‚ â–¸ Apply local filters   â”‚       â”‚ â–¸ Apply local filters       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â”‚                                     â”‚                â”‚
â”‚              â–¼                                     â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Kafka Scan              â”‚       â”‚ Kafka Scan                  â”‚  â”‚
â”‚  â”‚ â–¸ orders (partitions 0,1)â”‚      â”‚ â–¸ orders (partitions 2,3)  â”‚  â”‚
â”‚  â”‚ â–¸ Extract: customer_id, â”‚       â”‚ â–¸ Extract: customer_id,     â”‚  â”‚
â”‚  â”‚           amount        â”‚       â”‚           amount            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STAGE 2: DATA REDISTRIBUTION (SHUFFLE)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data gets redistributed by join key (customer_id)                 â”‚
â”‚                                                                     â”‚
â”‚  Worker 1 will get:                Worker 2 will get:              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ All customers where     â”‚       â”‚ All customers where         â”‚  â”‚
â”‚  â”‚ hash(id) % 2 = 0        â”‚       â”‚ hash(id) % 2 = 1            â”‚  â”‚
â”‚  â”‚                         â”‚       â”‚                             â”‚  â”‚
â”‚  â”‚ All orders where        â”‚       â”‚ All orders where            â”‚  â”‚
â”‚  â”‚ hash(customer_id) % 2=0 â”‚       â”‚ hash(customer_id) % 2 = 1   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â”‚  Network Transfer: Each worker sends data to correct destination   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Worker 1 â†’ Worker 1: customers(id=even), orders(cust_id=even)  â”‚ â”‚
â”‚  â”‚ Worker 1 â†’ Worker 2: customers(id=odd),  orders(cust_id=odd)   â”‚ â”‚
â”‚  â”‚ Worker 2 â†’ Worker 1: customers(id=even), orders(cust_id=even)  â”‚ â”‚
â”‚  â”‚ Worker 2 â†’ Worker 2: customers(id=odd),  orders(cust_id=odd)   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STAGE 3: LOCAL JOINS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Each worker performs local hash join on their subset              â”‚
â”‚                                                                     â”‚
â”‚  Worker 1:                          Worker 2:                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 1. Build hash table     â”‚       â”‚ 1. Build hash table         â”‚  â”‚
â”‚  â”‚    from customers       â”‚       â”‚    from customers           â”‚  â”‚
â”‚  â”‚    (key = id)           â”‚       â”‚    (key = id)               â”‚  â”‚
â”‚  â”‚                         â”‚       â”‚                             â”‚  â”‚
â”‚  â”‚ 2. Probe with orders    â”‚       â”‚ 2. Probe with orders        â”‚  â”‚
â”‚  â”‚    (key = customer_id)  â”‚       â”‚    (key = customer_id)      â”‚  â”‚
â”‚  â”‚                         â”‚       â”‚                             â”‚  â”‚
â”‚  â”‚ 3. Output joined rows   â”‚       â”‚ 3. Output joined rows       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STAGE 4: RESULT COLLECTION
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Coordinator collects results from all workers                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Worker 1 Results â†’ Coordinator                                  â”‚ â”‚
â”‚  â”‚ Worker 2 Results â†’ Coordinator                                  â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Optional final operations:                                      â”‚ â”‚
â”‚  â”‚ â–¸ Global ORDER BY                                              â”‚ â”‚
â”‚  â”‚ â–¸ LIMIT/OFFSET                                                 â”‚ â”‚
â”‚  â”‚ â–¸ Final projections                                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Join Algorithm Selection:**

#### **Hash Join (Most Common)**
- **When**: General purpose, most joins
- **How**: Build hash table from smaller side, probe with larger
- **Memory**: Builds hash table in memory
- **Performance**: O(M + N) where M, N are table sizes

#### **Broadcast Join**
- **When**: One side is very small (< 100MB)
- **How**: Send small table to all workers
- **Memory**: Small table replicated everywhere
- **Performance**: No shuffle needed, very fast

#### **Merge Join**
- **When**: Both sides are pre-sorted on join key
- **How**: Merge sorted streams
- **Memory**: Very low memory usage
- **Performance**: Good for very large tables

### **Aggregation Strategies:**

#### **Two-Phase Aggregation**
```
Phase 1 (Workers): Partial aggregation per worker
  Worker 1: GROUP BY customer_id â†’ partial counts
  Worker 2: GROUP BY customer_id â†’ partial counts

Phase 2 (Coordinator): Final aggregation
  Coordinator: Combine partial results â†’ final counts
```

#### **Hash-Based Distribution**
```
Distribute by grouping key:
  hash(customer_id) % num_workers = target_worker
  
Each worker gets complete groups for final aggregation
```

---

## ğŸ”’ Security Architecture

### **Your Current Security Setup**

Based on your configuration, here's how security works:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Security Architecture                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  CLIENT ACCESS                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ HTTP (not HTTPS) - Basic setup                                 â”‚ â”‚
â”‚  â”‚ â–¸ Port: 8081                                                   â”‚ â”‚
â”‚  â”‚ â–¸ No authentication required                                   â”‚ â”‚
â”‚  â”‚ â–¸ Internal network only (good for HomeLA)                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                  â”‚                                  â”‚
â”‚                                  â–¼                                  â”‚
â”‚  COORDINATOR                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ No built-in authentication (commented out)                     â”‚ â”‚
â”‚  â”‚ â–¸ http-server.authentication.type=PASSWORD (disabled)          â”‚ â”‚
â”‚  â”‚ â–¸ Access control via network (firewall, VPN)                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                  â”‚                                  â”‚
â”‚                                  â–¼                                  â”‚
â”‚  CONNECTOR-LEVEL SECURITY                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ PostgreSQL Connector:                                           â”‚ â”‚
â”‚  â”‚ â–¸ connection-user=dataeng                                       â”‚ â”‚
â”‚  â”‚ â–¸ connection-password=password                                  â”‚ â”‚
â”‚  â”‚ â–¸ Uses PostgreSQL's role-based access                          â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Kafka Connector:                                                â”‚ â”‚
â”‚  â”‚ â–¸ Direct connection to brokers                                  â”‚ â”‚
â”‚  â”‚ â–¸ No SASL/SSL configured                                       â”‚ â”‚
â”‚  â”‚ â–¸ Relies on network security                                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Security Best Practices for Production:**

#### **Network Security (Your Current Approach)**
- âœ… **Firewall**: Only allow 8081 from trusted networks
- âœ… **VPN**: Access Trino through VPN tunnel
- âœ… **Internal Network**: Keep cluster on private network

#### **Application-Level Security (Optional Upgrades)**
```bash
# Enable HTTPS
http-server.https.enabled=true
http-server.https.port=8443
http-server.https.keystore.path=/path/to/keystore.jks

# Enable authentication
http-server.authentication.type=PASSWORD
http-server.authentication.password.user-mapping.pattern=(.*)
```

#### **Data Source Security**
- **PostgreSQL**: Use dedicated service accounts with minimal privileges
- **Kafka**: Enable SASL/SCRAM authentication in production
- **File Systems**: Use service accounts with read-only access

---

## ğŸ“ˆ Scaling Your Setup

### **Current vs. Scaled Architecture**

Your current 3-node setup vs. what it could become:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CURRENT (3 nodes)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ cpu-node1   â”‚    â”‚ cpu-node2   â”‚    â”‚worker-node3 â”‚              â”‚
â”‚  â”‚(Coordinator)â”‚    â”‚ (Worker 1)  â”‚    â”‚ (Worker 2)  â”‚              â”‚
â”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚              â”‚
â”‚  â”‚ Memory: 2GB â”‚    â”‚ Memory: 2GB â”‚    â”‚ Memory: 2GB â”‚              â”‚
â”‚  â”‚ CPU: 2 coresâ”‚    â”‚ CPU: 2 coresâ”‚    â”‚ CPU: 2 coresâ”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                     â”‚
â”‚  Total Capacity: 6GB memory, 6 CPU cores                           â”‚
â”‚  Max Concurrent Queries: ~3 small or 1 large                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SCALED (10 nodes)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Coordinator â”‚    â”‚              Workers                      â”‚    â”‚
â”‚  â”‚             â”‚    â”‚                                           â”‚    â”‚
â”‚  â”‚ cpu-node1   â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚ Memory: 4GB â”‚    â”‚  â”‚Node2â”‚ â”‚Node3â”‚ â”‚Node4â”‚ â”‚Node5â”‚ â”‚Node6â”‚ â”‚    â”‚
â”‚  â”‚ CPU: 4 coresâ”‚    â”‚  â”‚ 8GB â”‚ â”‚ 8GB â”‚ â”‚ 8GB â”‚ â”‚ 8GB â”‚ â”‚ 8GB â”‚ â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚4coreâ”‚ â”‚4coreâ”‚ â”‚4coreâ”‚ â”‚4coreâ”‚ â”‚4coreâ”‚ â”‚    â”‚
â”‚                     â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚                     â”‚                                           â”‚    â”‚
â”‚                     â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”         â”‚    â”‚
â”‚                     â”‚  â”‚Node7â”‚ â”‚Node8â”‚ â”‚Node9â”‚ â”‚Nod10â”‚         â”‚    â”‚
â”‚                     â”‚  â”‚ 8GB â”‚ â”‚ 8GB â”‚ â”‚ 8GB â”‚ â”‚ 8GB â”‚         â”‚    â”‚
â”‚                     â”‚  â”‚4coreâ”‚ â”‚4coreâ”‚ â”‚4coreâ”‚ â”‚4coreâ”‚         â”‚    â”‚
â”‚                     â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜         â”‚    â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                     â”‚
â”‚  Total Capacity: 76GB memory, 40 CPU cores                         â”‚
â”‚  Max Concurrent Queries: ~30 small or 10 large                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Scaling Strategies:**

#### **Horizontal Scaling (Add More Workers)**
```bash
# Add new worker node (node4)
# 1. Install Trino on new node
# 2. Configure as worker:
coordinator=false
discovery.uri=http://192.168.1.184:8081

# 3. Start service
systemctl start trino

# 4. Worker auto-registers with coordinator
# 5. More parallelism automatically available
```

#### **Vertical Scaling (Bigger Machines)**
```bash
# Increase memory per node
query.max-memory-per-node=8GB
query.max-total-memory-per-node=12GB

# Increase task concurrency
task.concurrency=32
task.max-worker-threads=400
```

#### **Workload Isolation**
```bash
# Separate clusters for different workloads
Cluster 1: Interactive analytics (small, fast queries)
Cluster 2: ETL processing (large, batch queries)
Cluster 3: Reporting (scheduled, predictable queries)
```

### **When to Scale:**

#### **Add Workers When:**
- Queries frequently hit memory limits
- High CPU utilization across all workers
- Query queueing becomes common
- Need more parallelism for large tables

#### **Upgrade Coordinator When:**
- Many concurrent connections
- Complex query planning takes too long
- Metadata operations are slow
- Web UI becomes unresponsive

---

## âš¡ Performance Tuning

### **Tuning Your Current Setup**

Here are optimizations specific to your 3-node cluster:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Performance Tuning                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  COORDINATOR TUNING (cpu-node1)                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Memory Management:                                              â”‚ â”‚
â”‚  â”‚ â–¸ query.max-memory=6GB               (total across workers)    â”‚ â”‚
â”‚  â”‚ â–¸ query.max-queued-queries=1000      (queue size)              â”‚ â”‚
â”‚  â”‚ â–¸ query.max-concurrent-queries=10    (active queries)          â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Query Optimization:                                             â”‚ â”‚
â”‚  â”‚ â–¸ query.max-run-time=1h              (prevent runaway)         â”‚ â”‚
â”‚  â”‚ â–¸ query.low-memory-killer.delay=5m   (kill slow queries)       â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Planning Optimization:                                          â”‚ â”‚
â”‚  â”‚ â–¸ optimizer.join-reordering-strategy=AUTOMATIC                 â”‚ â”‚
â”‚  â”‚ â–¸ optimizer.join-distribution-type=AUTOMATIC                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                     â”‚
â”‚  WORKER TUNING (cpu-node2, worker-node3)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Task Execution:                                                 â”‚ â”‚
â”‚  â”‚ â–¸ task.concurrency=4                 (per worker)              â”‚ â”‚
â”‚  â”‚ â–¸ task.max-worker-threads=100        (thread pool)             â”‚ â”‚
â”‚  â”‚ â–¸ task.writer-count=1                (output writers)          â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Memory Configuration:                                           â”‚ â”‚
â”‚  â”‚ â–¸ query.max-memory-per-node=2GB      (per worker limit)        â”‚ â”‚
â”‚  â”‚ â–¸ memory.heap-headroom-per-node=512MB (JVM overhead)           â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Spill Configuration:                                            â”‚ â”‚
â”‚  â”‚ â–¸ spill-enabled=true                 (spill to disk)           â”‚ â”‚
â”‚  â”‚ â–¸ spiller-spill-path=/tmp/trino-spill (spill location)         â”‚ â”‚
â”‚  â”‚ â–¸ spiller-max-used-space-threshold=0.9 (disk usage limit)      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Connector-Specific Tuning:**

#### **PostgreSQL Connector Optimization**
```properties
# In your postgresql.properties:
connection-pool.max-size=50
connection-pool.max-connection-lifetime=2m
connection-pool.max-idle-time=1m

# Enable pushdown optimizations
postgresql.experimental.enable-predicate-pushdown-with-varchar-equality=true
postgresql.experimental.enable-predicate-pushdown-with-varchar-inequality=true
```

#### **Kafka Connector Optimization**
```properties
# In your kafka.properties:
kafka.buffer-size=1MB
kafka.messages-per-split=100000
kafka.consumer-timeout=1m

# For high-throughput topics
kafka.max-partition-fetch-bytes=1MB
kafka.fetch-min-bytes=1MB
```

### **Query-Level Optimizations:**

#### **Efficient Join Patterns**
```sql
-- âœ… GOOD: Filter before joining
SELECT c.name, SUM(o.amount)
FROM postgresql.customers c
JOIN kafka.orders o ON c.id = o.customer_id
WHERE c.country = 'USA'  -- Filter pushdown to PostgreSQL
  AND o.order_date >= DATE '2023-01-01'  -- Filter on Kafka
GROUP BY c.name;

-- âŒ BAD: Filter after joining
SELECT c.name, SUM(o.amount)
FROM postgresql.customers c
JOIN kafka.orders o ON c.id = o.customer_id
WHERE c.country = 'USA'  -- Lots of unnecessary data transferred
GROUP BY c.name;
```

#### **Memory-Efficient Aggregations**
```sql
-- âœ… GOOD: Use approximate functions for large datasets
SELECT customer_id, approx_distinct(product_id) as unique_products
FROM kafka.events
GROUP BY customer_id;

-- âŒ BAD: Exact distinct on large dataset
SELECT customer_id, COUNT(DISTINCT product_id) as unique_products
FROM kafka.events
GROUP BY customer_id;
```

### **Monitoring Performance:**

#### **Key Metrics to Watch**
```sql
-- Query execution time
SELECT query_id, state, elapsed_time_millis, queued_time_millis
FROM system.runtime.queries
WHERE state = 'RUNNING'
ORDER BY elapsed_time_millis DESC;

-- Memory usage per query
SELECT query_id, total_memory_reservation, peak_memory_reservation
FROM system.runtime.queries
WHERE state = 'RUNNING'
ORDER BY peak_memory_reservation DESC;

-- Worker utilization
SELECT node_id, processor_count, heap_available, heap_used
FROM system.runtime.nodes;
```

#### **Performance Troubleshooting Queries**
```sql
-- Find slow queries
SELECT query_id, query, state, total_cpu_time, total_scheduled_time
FROM system.runtime.queries
WHERE total_cpu_time > INTERVAL '30' SECOND
ORDER BY total_cpu_time DESC;

-- Check for memory pressure
SELECT query_id, memory_reservation, peak_memory_reservation
FROM system.runtime.queries
WHERE memory_reservation > 1000000000  -- > 1GB
ORDER BY peak_memory_reservation DESC;

-- Analyze failed queries
SELECT query_id, error_type, error_message
FROM system.runtime.queries
WHERE state = 'FAILED'
ORDER BY created DESC
LIMIT 10;
```

---

## ğŸ—ï¸ Architecture Changes When Adding Nodes

### **Evolution of Your Cluster**

Here's how your architecture changes as you add more nodes:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      3-Node Setup (Current)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Single Coordinator + 2 Workers                                 â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Characteristics:                                                â”‚ â”‚
â”‚  â”‚ â–¸ Simple setup and management                                  â”‚ â”‚
â”‚  â”‚ â–¸ All queries routed through one coordinator                   â”‚ â”‚
â”‚  â”‚ â–¸ Limited parallelism (2 workers)                              â”‚ â”‚
â”‚  â”‚ â–¸ Good for: Small teams, development, light analytics          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      5-7 Node Setup (Small Scale)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Single Coordinator + 4-6 Workers                               â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Characteristics:                                                â”‚ â”‚
â”‚  â”‚ â–¸ Better parallelism for medium queries                        â”‚ â”‚
â”‚  â”‚ â–¸ Can handle more concurrent users                             â”‚ â”‚
â”‚  â”‚ â–¸ Coordinator may become bottleneck                            â”‚ â”‚
â”‚  â”‚ â–¸ Good for: Department-level analytics                         â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Configuration Changes:                                          â”‚ â”‚
â”‚  â”‚ â–¸ Increase coordinator memory (4-8GB)                          â”‚ â”‚
â”‚  â”‚ â–¸ Tune task.concurrency per worker                             â”‚ â”‚
â”‚  â”‚ â–¸ Enable spilling for large queries                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     10+ Node Setup (Large Scale)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Dedicated Coordinator + Many Workers                           â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Characteristics:                                                â”‚ â”‚
â”‚  â”‚ â–¸ Coordinator doesn't run worker tasks                         â”‚ â”‚
â”‚  â”‚ â–¸ High parallelism for complex queries                         â”‚ â”‚
â”‚  â”‚ â–¸ Can handle many concurrent users                             â”‚ â”‚
â”‚  â”‚ â–¸ Good for: Enterprise-wide analytics                          â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Configuration Changes:                                          â”‚ â”‚
â”‚  â”‚ â–¸ coordinator=true, node-scheduler.include-coordinator=false   â”‚ â”‚
â”‚  â”‚ â–¸ Dedicated coordinator hardware                               â”‚ â”‚
â”‚  â”‚ â–¸ Resource groups for workload isolation                       â”‚ â”‚
â”‚  â”‚ â–¸ Query queuing and admission control                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   High Availability Setup                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Multiple Coordinators + Load Balancer                          â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚ â”‚
â”‚  â”‚      â”‚Load Balancerâ”‚                                           â”‚ â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚ â”‚
â”‚  â”‚           â”‚                                                     â”‚ â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                                             â”‚ â”‚
â”‚  â”‚    â”‚             â”‚                                             â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â–¼â”€â”€â”       â”Œâ”€â”€â–¼â”€â”€â”                                          â”‚ â”‚
â”‚  â”‚ â”‚Coordâ”‚       â”‚Coordâ”‚                                          â”‚ â”‚
â”‚  â”‚ â”‚  1  â”‚       â”‚  2  â”‚                                          â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”˜                                          â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚ Characteristics:                                                â”‚ â”‚
â”‚  â”‚ â–¸ No single point of failure                                   â”‚ â”‚
â”‚  â”‚ â–¸ Automatic failover                                           â”‚ â”‚
â”‚  â”‚ â–¸ Horizontal scaling of query planning                         â”‚ â”‚
â”‚  â”‚ â–¸ Good for: Mission-critical systems                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Adding Nodes to Your Current Setup:**

#### **Step 1: Add Worker Node (Easiest)**
```bash
# On new node (e.g., node4)
# 1. Install Trino
sudo apt install openjdk-17-jdk
# Download and extract Trino...

# 2. Configure as worker
nano /home/trino/trino/etc/config.properties
coordinator=false
http-server.http.port=8081
discovery.uri=http://192.168.1.184:8081  # Points to your coordinator

# 3. Copy catalog configurations
scp -r cpu-node1:/home/trino/trino/etc/catalog/* /home/trino/trino/etc/catalog/

# 4. Start service
systemctl start trino

# 5. Verify in coordinator web UI (http://192.168.1.184:8081)
# Should see new worker auto-registered
```

#### **Step 2: Resource Adjustments**
```bash
# Update total memory limits
query.max-memory=9GB  # Was 6GB, now 6GB + 3GB for new worker
query.max-memory-per-node=3GB  # Increase per-node limit
```

#### **Step 3: Optimize for More Workers**
```bash
# Increase parallelism
task.concurrency=8  # More tasks per worker
task.max-worker-threads=200  # More threads

# Better memory management
spill-enabled=true
spiller-threads=8
```

### **What Changes When You Scale:**

#### **Query Distribution Patterns**
```
3 Workers: Simple hash distribution
  hash(key) % 3 = {0, 1, 2}

6 Workers: Better granularity
  hash(key) % 6 = {0, 1, 2, 3, 4, 5}

12 Workers: Very fine-grained
  hash(key) % 12 = {0, 1, 2, ..., 11}
```

#### **Memory Considerations**
```
More Workers = More Total Memory = Larger Queries Possible

3 Workers Ã— 2GB = 6GB total â†’ Can handle 6GB queries
6 Workers Ã— 3GB = 18GB total â†’ Can handle 18GB queries
12 Workers Ã— 4GB = 48GB total â†’ Can handle 48GB queries
```

#### **Network Traffic Patterns**
```
More Workers = More Shuffle Traffic

3-node joins: 3Ã—3 = 9 network connections
6-node joins: 6Ã—6 = 36 network connections
12-node joins: 12Ã—12 = 144 network connections

â†’ Network becomes more important with scale
```

---

## ğŸ¯ Summary: Your Trino Architecture

### **What You Have Today**

Your 3-node Trino cluster provides:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Current Capabilities                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  âœ… Multi-Source Queries                                           â”‚
â”‚     â–¸ Join PostgreSQL + Kafka + Iceberg + Delta Lake in one SQL    â”‚ â”‚
â”‚     â–¸ No ETL needed - query data where it lives                    â”‚ â”‚
â”‚                                                                     â”‚
â”‚  âœ… Interactive Performance                                         â”‚
â”‚     â–¸ Sub-second queries on small datasets                         â”‚ â”‚
â”‚     â–¸ Minute-scale queries on medium datasets                      â”‚ â”‚
â”‚     â–¸ Distributed parallel execution                               â”‚ â”‚
â”‚                                                                     â”‚
â”‚  âœ… SQL Analytics Power                                             â”‚
â”‚     â–¸ Complex JOINs across different systems                       â”‚ â”‚
â”‚     â–¸ Window functions, CTEs, advanced analytics                   â”‚ â”‚
â”‚     â–¸ Approximate functions for big data                           â”‚ â”‚
â”‚                                                                     â”‚
â”‚  âœ… Easy Management                                                 â”‚
â”‚     â–¸ Simple systemd services (no SSH complexity)                  â”‚ â”‚
â”‚     â–¸ HTTP-based communication                                     â”‚ â”‚
â”‚     â–¸ Auto-discovery and self-registration                         â”‚ â”‚
â”‚                                                                     â”‚
â”‚  âœ… Integration Ready                                               â”‚
â”‚     â–¸ Connects to your existing PostgreSQL, Kafka                  â”‚ â”‚
â”‚     â–¸ Ready for Iceberg and Delta Lake tables                      â”‚ â”‚
â”‚     â–¸ BI tools can connect via JDBC/ODBC                           â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Key Architectural Principles**

1. **Coordinator-Worker Model**: Single brain, multiple hands
2. **HTTP Communication**: No SSH complexity like Spark/Flink
3. **Connector Architecture**: Pluggable data source access
4. **Memory-Centric**: Optimized for interactive queries
5. **Cost-Based Optimization**: Smart query planning across systems

### **When to Use Trino vs. Other Tools**

**Use Trino for:**
- âœ… Interactive analytics and dashboards
- âœ… Cross-system joins and federation
- âœ… Ad-hoc data exploration
- âœ… Real-time reporting

**Use Spark for:**
- ğŸ”„ ETL and data transformation jobs
- ğŸ”„ Machine learning pipelines
- ğŸ”„ Batch processing workflows

**Use Flink for:**
- ğŸ”„ Real-time stream processing
- ğŸ”„ Event-driven applications
- ğŸ”„ Low-latency data pipelines

### **Your Next Steps**

1. **Start Simple**: Use your current 3-node setup for analytics
2. **Monitor Performance**: Watch memory usage and query times
3. **Scale Horizontally**: Add workers as query load increases
4. **Optimize Queries**: Use connector pushdown and efficient joins
5. **Consider HA**: Add coordinator redundancy for production

Your Trino cluster is a powerful addition to your data engineering stack - perfect for the "analytics" layer that sits on top of your Kafka streams, PostgreSQL databases, and lakehouse tables! ğŸš€

---

*This guide covers the architecture of **your specific Trino setup**. For production deployments, consider additional topics like high availability, security hardening, and advanced performance tuning.*
